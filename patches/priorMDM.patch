diff --git a/.gitignore b/.gitignore
index d036dfa..010d2b9 100644
--- a/.gitignore
+++ b/.gitignore
@@ -129,3 +129,11 @@ dmypy.json
 
 # Pyre type checker
 .pyre/
+
+# Datasets and results
+/t2m
+/save
+/kit
+/glove
+/body_models/smpl
+/dataset
diff --git a/__init__.py b/__init__.py
new file mode 100644
index 0000000..aff1f16
--- /dev/null
+++ b/__init__.py
@@ -0,0 +1 @@
+__package__ = "priorMDM"
diff --git a/data_loaders/amass/amass_split_babel.py b/data_loaders/amass/amass_split_babel.py
index 288370e..02d8795 100644
--- a/data_loaders/amass/amass_split_babel.py
+++ b/data_loaders/amass/amass_split_babel.py
@@ -17,7 +17,7 @@
 import joblib
 import sys
 
-from data_loaders.amass.file_io import read_json
+from .file_io import read_json
 
 sys.path.append('.')
 
diff --git a/data_loaders/amass/babel.py b/data_loaders/amass/babel.py
index 6061249..311fd02 100644
--- a/data_loaders/amass/babel.py
+++ b/data_loaders/amass/babel.py
@@ -535,7 +535,7 @@ class BABEL(Dataset):
                         num_bad_actions += 1
                         continue
 
-                from data_loaders.amass.tools.smpl import smpl_data_to_matrix_and_trans
+                from .tools.smpl import smpl_data_to_matrix_and_trans
                 # here we take smpl data in poses/trans and move to rots/trans
                 if self.sep_to_4:
                     if frames_0_valid:
diff --git a/data_loaders/amass/tools/smpl.py b/data_loaders/amass/tools/smpl.py
index 671b88c..7887541 100644
--- a/data_loaders/amass/tools/smpl.py
+++ b/data_loaders/amass/tools/smpl.py
@@ -18,8 +18,8 @@ from typing import Optional
 import torch
 from torch import Tensor
 from data_loaders.amass.tools_teach import geometry
-from data_loaders.amass.tools_teach.easyconvert import axis_angle_to
-from data_loaders.amass.transforms.smpl import RotTransDatastruct
+from ..tools_teach.easyconvert import axis_angle_to
+from ..transforms.smpl import RotTransDatastruct
 
 def canonicalize_smplh(poses: Tensor, trans: Optional[Tensor] = None):
     bs, nframes, njoints = poses.shape[:3]
diff --git a/data_loaders/amass/tools_teach/easyconvert.py b/data_loaders/amass/tools_teach/easyconvert.py
index 3b0c0a2..0557540 100644
--- a/data_loaders/amass/tools_teach/easyconvert.py
+++ b/data_loaders/amass/tools_teach/easyconvert.py
@@ -14,7 +14,7 @@
 #
 # Contact: ps-license@tuebingen.mpg.de
 
-from data_loaders.amass.tools_teach import geometry
+from priorMDM.data_loaders.amass.tools_teach import geometry
 
 
 def nfeats_of(rottype):
diff --git a/data_loaders/amass/transforms/joints2jfeats/tools.py b/data_loaders/amass/transforms/joints2jfeats/tools.py
index bc4a4f2..7767096 100644
--- a/data_loaders/amass/transforms/joints2jfeats/tools.py
+++ b/data_loaders/amass/transforms/joints2jfeats/tools.py
@@ -28,7 +28,7 @@ import torch.nn.functional as F
 
 # Get the indexes of particular body part
 # Feet
-from data_loaders.amass.info.joints import mmm_joints
+from ...info.joints import mmm_joints
 
 LM, RM = mmm_joints.index("LMrot"), mmm_joints.index("RMrot")
 LF, RF = mmm_joints.index("LF"), mmm_joints.index("RF")
diff --git a/data_loaders/amass/transforms/rots2rfeats/globvelandy.py b/data_loaders/amass/transforms/rots2rfeats/globvelandy.py
index 891f723..affb5bf 100644
--- a/data_loaders/amass/transforms/rots2rfeats/globvelandy.py
+++ b/data_loaders/amass/transforms/rots2rfeats/globvelandy.py
@@ -119,5 +119,5 @@ class Globalvelandy(Rots2Rfeats):
         matrix_poses = to_matrix(self.pose_rep, poses)
 
         # from teach.transforms.smpl import RotTransDatastruct
-        from data_loaders.amass.transforms.smpl import RotTransDatastruct
+        from ..smpl import RotTransDatastruct
         return RotTransDatastruct(rots=matrix_poses, trans=trans)
diff --git a/data_loaders/get_data.py b/data_loaders/get_data.py
index a83f115..525de7a 100644
--- a/data_loaders/get_data.py
+++ b/data_loaders/get_data.py
@@ -1,46 +1,58 @@
 from torch.utils.data import DataLoader
 
-from data_loaders.amass.sampling import FrameSampler
-from data_loaders.tensors import collate as all_collate, babel_collate, babel_eval_collate, pw3d_collate
-from data_loaders.tensors import t2m_collate
+from priorMDM.data_loaders.amass.sampling import FrameSampler
+from .tensors import (
+    collate as all_collate,
+    babel_collate,
+    babel_eval_collate,
+    pw3d_collate,
+)
+from .tensors import t2m_collate
 from torch.utils.data._utils.collate import default_collate
-from data_loaders.humanml.data.dataset import collate_fn as sorted_collate
+from .humanml.data.dataset import collate_fn as sorted_collate
+
 
 def get_dataset_class(name, load_mode):
     if name == "babel":
         if load_mode == "text_only":
-            load_mode = 'train'
-        if load_mode in ['gt', 'eval', 'movement_train', 'evaluator_train']:
-            from data_loaders.humanml.data.dataset import BABEL_eval
+            load_mode = "train"
+        if load_mode in ["gt", "eval", "movement_train", "evaluator_train"]:
+            from .humanml.data.dataset import BABEL_eval
+
             return BABEL_eval
-        elif load_mode == 'train':
-            from data_loaders.amass.babel import BABEL
+        elif load_mode == "train":
+            from .amass.babel import BABEL
+
             return BABEL
         else:
-            raise ValueError(f'Unsupported load_moad name [{load_mode}]')
+            raise ValueError(f"Unsupported load_moad name [{load_mode}]")
     elif name == "humanml":
-        from data_loaders.humanml.data.dataset import HumanML3D
+        from .humanml.data.dataset import HumanML3D
+
         return HumanML3D
     elif name == "pw3d":
-        from data_loaders.humanml.data.dataset import PW3D
+        from .humanml.data.dataset import PW3D
+
         return PW3D
     else:
-        raise ValueError(f'Unsupported dataset name [{name}]')
+        raise ValueError(f"Unsupported dataset name [{name}]")
+
+
+def get_collate_fn(name, load_mode="train"):
+    if load_mode == "gt":
+        from .humanml.data.dataset import collate_fn as t2m_eval_collate
 
-def get_collate_fn(name, load_mode='train'):
-    if load_mode == 'gt':
-        from data_loaders.humanml.data.dataset import collate_fn as t2m_eval_collate
         return t2m_eval_collate
     if name in ["humanml", "kit"]:
         return t2m_collate
-    elif name == 'pw3d':
+    elif name == "pw3d":
         return pw3d_collate
-    elif name == 'babel':
-        if load_mode =='eval':
+    elif name == "babel":
+        if load_mode == "eval":
             return babel_eval_collate
-        elif load_mode == 'movement_train':
+        elif load_mode == "movement_train":
             return default_collate
-        elif load_mode == 'evaluator_train':
+        elif load_mode == "evaluator_train":
             return sorted_collate
         else:
             return babel_collate
@@ -48,37 +60,91 @@ def get_collate_fn(name, load_mode='train'):
         return all_collate
 
 
-def get_dataset(name, num_frames, split='train', load_mode='train', batch_size=None, opt=None, short_db=False, cropping_sampler=False, size=None):
+def get_dataset(
+    name,
+    num_frames,
+    split="train",
+    load_mode="train",
+    batch_size=None,
+    opt=None,
+    short_db=False,
+    cropping_sampler=False,
+    size=None,
+):
     DATA = get_dataset_class(name, load_mode)
     if name in ["humanml", "pw3d"]:
-        dataset = DATA(split=split, num_frames=num_frames, load_mode=load_mode, size=size)
+        dataset = DATA(
+            split=split, num_frames=num_frames, load_mode=load_mode, size=size
+        )
     elif name == "babel":
         from data_loaders.amass.transforms import SlimSMPLTransform
-        if ((split=='val') and (cropping_sampler==True)):
-            transform = SlimSMPLTransform(batch_size=batch_size, name='SlimSMPLTransform', ename='smplnh',
-                                          normalization=True, canonicalize=False)
+
+        if (split == "val") and (cropping_sampler == True):
+            transform = SlimSMPLTransform(
+                batch_size=batch_size,
+                name="SlimSMPLTransform",
+                ename="smplnh",
+                normalization=True,
+                canonicalize=False,
+            )
         else:
-            transform = SlimSMPLTransform(batch_size=batch_size, name='SlimSMPLTransform', ename='smplnh', normalization=True)
+            transform = SlimSMPLTransform(
+                batch_size=batch_size,
+                name="SlimSMPLTransform",
+                ename="smplnh",
+                normalization=True,
+            )
         sampler = FrameSampler(min_len=num_frames[0], max_len=num_frames[1])
-        dataset = DATA(split=split,
-                       datapath='./dataset/babel/babel-smplh-30fps-male',
-                       transforms=transform, load_mode=load_mode, mode='train', opt=opt, sampler=sampler,
-                       short_db=short_db, cropping_sampler=cropping_sampler)
+        dataset = DATA(
+            split=split,
+            datapath="./dataset/babel/babel-smplh-30fps-male",
+            transforms=transform,
+            load_mode=load_mode,
+            mode="train",
+            opt=opt,
+            sampler=sampler,
+            short_db=short_db,
+            cropping_sampler=cropping_sampler,
+        )
     else:
         dataset = DATA(split=split, num_frames=num_frames)
     return dataset
 
 
-def get_dataset_loader(name, batch_size, num_frames, split='train', load_mode='train', opt=None, short_db=False, cropping_sampler=False, size=None):
-    if load_mode == 'text_only':
-        load_mode = 'train'
-    dataset = get_dataset(name, num_frames, split, load_mode, batch_size, opt, short_db, cropping_sampler, size)
+def get_dataset_loader(
+    name,
+    batch_size,
+    num_frames,
+    split="train",
+    load_mode="train",
+    opt=None,
+    short_db=False,
+    cropping_sampler=False,
+    size=None,
+):
+    if load_mode == "text_only":
+        load_mode = "train"
+    dataset = get_dataset(
+        name,
+        num_frames,
+        split,
+        load_mode,
+        batch_size,
+        opt,
+        short_db,
+        cropping_sampler,
+        size,
+    )
     collate = get_collate_fn(name, load_mode)
 
-    n_workers = 1 if load_mode in ['movement_train', 'evaluator_train'] else 8
+    n_workers = 1 if load_mode in ["movement_train", "evaluator_train"] else 8
     loader = DataLoader(
-        dataset, batch_size=batch_size, shuffle=True,
-        num_workers=n_workers, drop_last=True, collate_fn=collate
+        dataset,
+        batch_size=batch_size,
+        shuffle=True,
+        num_workers=n_workers,
+        drop_last=True,
+        collate_fn=collate,
     )
 
-    return loader
\ No newline at end of file
+    return loader
diff --git a/data_loaders/humanml/collect_babel_stats.py b/data_loaders/humanml/collect_babel_stats.py
index bdde646..8141c08 100644
--- a/data_loaders/humanml/collect_babel_stats.py
+++ b/data_loaders/humanml/collect_babel_stats.py
@@ -1,4 +1,4 @@
-from data_loaders.get_data import get_dataset_loader
+from ..get_data import get_dataset_loader
 import numpy as np
 from tqdm import tqdm
 import os
diff --git a/data_loaders/humanml/common/skeleton.py b/data_loaders/humanml/common/skeleton.py
index ceaad10..b2ae85a 100644
--- a/data_loaders/humanml/common/skeleton.py
+++ b/data_loaders/humanml/common/skeleton.py
@@ -1,4 +1,4 @@
-from data_loaders.humanml.common.quaternion import *
+from .quaternion import *
 import scipy.ndimage.filters as filters
 
 class Skeleton(object):
diff --git a/data_loaders/humanml/data/dataset.py b/data_loaders/humanml/data/dataset.py
index f3f7677..1b31198 100644
--- a/data_loaders/humanml/data/dataset.py
+++ b/data_loaders/humanml/data/dataset.py
@@ -2,47 +2,52 @@ import torch
 from torch.utils import data
 import numpy as np
 import os
-from os.path import join as pjoin
+from os.path import join as pjoin, dirname
 import random
 import codecs as cs
 from tqdm import tqdm
 import spacy
 import itertools
+import logging
 
 from torch.utils.data._utils.collate import default_collate
-from data_loaders.amass.sampling import FrameSampler
-from data_loaders.humanml.utils.word_vectorizer import WordVectorizer
-from data_loaders.humanml.utils.get_opt import get_opt
+from ..utils.word_vectorizer import WordVectorizer
+from ..utils.get_opt import get_opt
 
-from data_loaders.amass.babel import BABEL
+from ...amass.babel import BABEL
+from inspect import currentframe, getframeinfo
 
+logger = logging.getLogger(__name__)
 
 MOTION_TYPES = [
-    '_0',
-    '_1',
-    '_0_with_transition',
-    '_1_with_transition',
+    "_0",
+    "_1",
+    "_0_with_transition",
+    "_1_with_transition",
 ]
 
+
 def collate_fn(batch):
     batch.sort(key=lambda x: x[3], reverse=True)
     return default_collate(batch)
 
 
-'''For use of training text-2-motion generative model'''
+"""For use of training text-2-motion generative model"""
+
+
 class Text2MotionDataset(data.Dataset):
     def __init__(self, opt, mean, std, split_file, w_vectorizer):
         self.opt = opt
         self.w_vectorizer = w_vectorizer
         self.max_length = 20
         self.pointer = 0
-        min_motion_len = 40 if self.opt.dataset_name =='t2m' else 24
+        min_motion_len = 40 if self.opt.dataset_name == "t2m" else 24
 
         joints_num = opt.joints_num
 
         data_dict = {}
         id_list = []
-        with cs.open(split_file, 'r') as f:
+        with cs.open(split_file, "r") as f:
             for line in f.readlines():
                 id_list.append(line.strip())
 
@@ -50,38 +55,50 @@ class Text2MotionDataset(data.Dataset):
         length_list = []
         for name in tqdm(id_list):
             try:
-                motion = np.load(pjoin(opt.motion_dir, name + '.npy'))
+                motion = np.load(pjoin(opt.motion_dir, name + ".npy"))
                 if (len(motion)) < min_motion_len or (len(motion) >= 200):
                     continue
                 text_data = []
                 flag = False
-                with cs.open(pjoin(opt.text_dir, name + '.txt')) as f:
+                with cs.open(pjoin(opt.text_dir, name + ".txt")) as f:
                     for line in f.readlines():
                         text_dict = {}
-                        line_split = line.strip().split('#')
+                        line_split = line.strip().split("#")
                         caption = line_split[0]
-                        tokens = line_split[1].split(' ')
+                        tokens = line_split[1].split(" ")
                         f_tag = float(line_split[2])
                         to_tag = float(line_split[3])
                         f_tag = 0.0 if np.isnan(f_tag) else f_tag
                         to_tag = 0.0 if np.isnan(to_tag) else to_tag
 
-                        text_dict['caption'] = caption
-                        text_dict['tokens'] = tokens
+                        text_dict["caption"] = caption
+                        text_dict["tokens"] = tokens
                         if f_tag == 0.0 and to_tag == 0.0:
                             flag = True
                             text_data.append(text_dict)
                         else:
                             try:
-                                n_motion = motion[int(f_tag*20) : int(to_tag*20)]
-                                if (len(n_motion)) < min_motion_len or (len(n_motion) >= 200):
+                                n_motion = motion[int(f_tag * 20) : int(to_tag * 20)]
+                                if (len(n_motion)) < min_motion_len or (
+                                    len(n_motion) >= 200
+                                ):
                                     continue
-                                new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
+                                new_name = (
+                                    random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                    + "_"
+                                    + name
+                                )
                                 while new_name in data_dict:
-                                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
-                                data_dict[new_name] = {'motion': n_motion,
-                                                       'length': len(n_motion),
-                                                       'text': [text_dict]}
+                                    new_name = (
+                                        random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                        + "_"
+                                        + name
+                                    )
+                                data_dict[new_name] = {
+                                    "motion": n_motion,
+                                    "length": len(n_motion),
+                                    "text": [text_dict],
+                                }
                                 new_name_list.append(new_name)
                                 length_list.append(len(n_motion))
                             except:
@@ -90,17 +107,20 @@ class Text2MotionDataset(data.Dataset):
                                 # break
 
                 if flag:
-                    data_dict[name] = {'motion': motion,
-                                       'length': len(motion),
-                                       'text':text_data}
+                    data_dict[name] = {
+                        "motion": motion,
+                        "length": len(motion),
+                        "text": text_data,
+                    }
                     new_name_list.append(name)
                     length_list.append(len(motion))
             except:
                 # Some motion may not exist in KIT dataset
                 pass
 
-
-        name_list, length_list = zip(*sorted(zip(new_name_list, length_list), key=lambda x: x[1]))
+        name_list, length_list = zip(
+            *sorted(zip(new_name_list, length_list), key=lambda x: x[1])
+        )
 
         if opt.is_train:
             # root_rot_velocity (B, seq_len, 1)
@@ -110,21 +130,28 @@ class Text2MotionDataset(data.Dataset):
             # root_y (B, seq_len, 1)
             std[3:4] = std[3:4] / opt.feat_bias
             # ric_data (B, seq_len, (joint_num - 1)*3)
-            std[4: 4 + (joints_num - 1) * 3] = std[4: 4 + (joints_num - 1) * 3] / 1.0
+            std[4 : 4 + (joints_num - 1) * 3] = std[4 : 4 + (joints_num - 1) * 3] / 1.0
             # rot_data (B, seq_len, (joint_num - 1)*6)
-            std[4 + (joints_num - 1) * 3: 4 + (joints_num - 1) * 9] = std[4 + (joints_num - 1) * 3: 4 + (
-                        joints_num - 1) * 9] / 1.0
+            std[4 + (joints_num - 1) * 3 : 4 + (joints_num - 1) * 9] = (
+                std[4 + (joints_num - 1) * 3 : 4 + (joints_num - 1) * 9] / 1.0
+            )
             # local_velocity (B, seq_len, joint_num*3)
-            std[4 + (joints_num - 1) * 9: 4 + (joints_num - 1) * 9 + joints_num * 3] = std[
-                                                                                       4 + (joints_num - 1) * 9: 4 + (
-                                                                                                   joints_num - 1) * 9 + joints_num * 3] / 1.0
+            std[
+                4 + (joints_num - 1) * 9 : 4 + (joints_num - 1) * 9 + joints_num * 3
+            ] = (
+                std[
+                    4 + (joints_num - 1) * 9 : 4 + (joints_num - 1) * 9 + joints_num * 3
+                ]
+                / 1.0
+            )
             # foot contact (B, seq_len, 4)
-            std[4 + (joints_num - 1) * 9 + joints_num * 3:] = std[
-                                                              4 + (joints_num - 1) * 9 + joints_num * 3:] / opt.feat_bias
+            std[4 + (joints_num - 1) * 9 + joints_num * 3 :] = (
+                std[4 + (joints_num - 1) * 9 + joints_num * 3 :] / opt.feat_bias
+            )
 
             assert 4 + (joints_num - 1) * 9 + joints_num * 3 + 4 == mean.shape[-1]
-            np.save(pjoin(opt.meta_dir, 'mean.npy'), mean)
-            np.save(pjoin(opt.meta_dir, 'std.npy'), std)
+            np.save(pjoin(opt.meta_dir, "mean.npy"), mean)
+            np.save(pjoin(opt.meta_dir, "std.npy"), std)
 
         self.mean = mean
         self.std = std
@@ -136,7 +163,7 @@ class Text2MotionDataset(data.Dataset):
     def reset_max_len(self, length):
         assert length <= self.opt.max_motion_length
         self.pointer = np.searchsorted(self.length_arr, length)
-        print("Pointer Pointing at %d"%self.pointer)
+        logger.debug("Pointer Pointing at %d" % self.pointer)
         self.max_length = length
 
     def inv_transform(self, data):
@@ -148,20 +175,20 @@ class Text2MotionDataset(data.Dataset):
     def __getitem__(self, item):
         idx = self.pointer + item
         data = self.data_dict[self.name_list[idx]]
-        motion, m_length, text_list = data['motion'], data['length'], data['text']
+        motion, m_length, text_list = data["motion"], data["length"], data["text"]
         # Randomly select a caption
         text_data = random.choice(text_list)
-        caption, tokens = text_data['caption'], text_data['tokens']
+        caption, tokens = text_data["caption"], text_data["tokens"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -176,36 +203,38 @@ class Text2MotionDataset(data.Dataset):
 
         if self.opt.is_train:
             if m_length != self.max_length:
-            # print("Motion original length:%d_%d"%(m_length, len(motion)))
+                # print("Motion original length:%d_%d"%(m_length, len(motion)))
                 if self.opt.unit_length < 10:
-                    coin2 = np.random.choice(['single', 'single', 'double'])
+                    coin2 = np.random.choice(["single", "single", "double"])
                 else:
-                    coin2 = 'single'
-                if len_gap == 0 or (len_gap == 1 and coin2 == 'double'):
+                    coin2 = "single"
+                if len_gap == 0 or (len_gap == 1 and coin2 == "double"):
                     m_length = self.max_length
                     idx = random.randint(0, m_length - self.max_length)
-                    motion = motion[idx:idx+self.max_length]
+                    motion = motion[idx : idx + self.max_length]
                 else:
-                    if coin2 == 'single':
+                    if coin2 == "single":
                         n_m_length = self.max_length + self.opt.unit_length * len_gap
                     else:
-                        n_m_length = self.max_length + self.opt.unit_length * (len_gap - 1)
+                        n_m_length = self.max_length + self.opt.unit_length * (
+                            len_gap - 1
+                        )
                     idx = random.randint(0, m_length - n_m_length)
-                    motion = motion[idx:idx + self.max_length]
+                    motion = motion[idx : idx + self.max_length]
                     m_length = n_m_length
                 # print(len_gap, idx, coin2)
         else:
             if self.opt.unit_length < 10:
-                coin2 = np.random.choice(['single', 'single', 'double'])
+                coin2 = np.random.choice(["single", "single", "double"])
             else:
-                coin2 = 'single'
+                coin2 = "single"
 
-            if coin2 == 'double':
+            if coin2 == "double":
                 m_length = (m_length // self.opt.unit_length - 1) * self.opt.unit_length
-            elif coin2 == 'single':
+            elif coin2 == "single":
                 m_length = (m_length // self.opt.unit_length) * self.opt.unit_length
             idx = random.randint(0, len(motion) - m_length)
-            motion = motion[idx:idx+m_length]
+            motion = motion[idx : idx + m_length]
 
         "Z Normalization"
         motion = (motion - self.mean) / self.std
@@ -213,23 +242,27 @@ class Text2MotionDataset(data.Dataset):
         return word_embeddings, pos_one_hots, caption, sent_len, motion, m_length
 
 
-'''For use of training text motion matching model, and evaluations'''
+"""For use of training text motion matching model, and evaluations"""
+
+
 class Text2MotionDatasetV2(data.Dataset):
-    def __init__(self, opt, mean, std, split_file, w_vectorizer, num_frames, size=None, **kwargs):
+    def __init__(
+        self, opt, mean, std, split_file, w_vectorizer, num_frames, size=None, **kwargs
+    ):
         self.opt = opt
         self.w_vectorizer = w_vectorizer
         self.max_length = 20
         self.pointer = 0
         self.num_frames = num_frames if num_frames else False
         self.max_motion_length = opt.max_motion_length
-        if (self.num_frames == False) or type(self.num_frames)==int:
-            min_motion_len = 40 if self.opt.dataset_name =='t2m' else 24
+        if (self.num_frames == False) or type(self.num_frames) == int:
+            min_motion_len = 40 if self.opt.dataset_name == "t2m" else 24
         else:
             min_motion_len = self.num_frames[0]
             self.max_motion_length = self.num_frames[1]
         data_dict = {}
         id_list = []
-        with cs.open(split_file, 'r') as f:
+        with cs.open(split_file, "r") as f:
             for line in f.readlines():
                 id_list.append(line.strip())
         id_list = id_list[:size]
@@ -238,53 +271,73 @@ class Text2MotionDatasetV2(data.Dataset):
         length_list = []
         for name in tqdm(id_list):
             try:
-                motion = np.load(pjoin(opt.motion_dir, name + '.npy'))
+                motion = np.load(pjoin(opt.motion_dir, name + ".npy"))
                 if (len(motion)) < min_motion_len or (len(motion) >= 200):
                     continue
                 text_data = []
                 flag = False
-                with cs.open(pjoin(opt.text_dir, name + '.txt')) as f:
+                with cs.open(pjoin(opt.text_dir, name + ".txt")) as f:
                     for line in f.readlines():
                         text_dict = {}
-                        line_split = line.strip().split('#')
+                        line_split = line.strip().split("#")
                         caption = line_split[0]
-                        tokens = line_split[1].split(' ')
+                        tokens = line_split[1].split(" ")
                         f_tag = float(line_split[2])
                         to_tag = float(line_split[3])
                         f_tag = 0.0 if np.isnan(f_tag) else f_tag
                         to_tag = 0.0 if np.isnan(to_tag) else to_tag
 
-                        text_dict['caption'] = caption
-                        text_dict['tokens'] = tokens
+                        text_dict["caption"] = caption
+                        text_dict["tokens"] = tokens
                         if f_tag == 0.0 and to_tag == 0.0:
                             flag = True
                             text_data.append(text_dict)
                         else:
                             try:
-                                n_motion = motion[int(f_tag*20) : int(to_tag*20)]
-                                if (len(n_motion)) < min_motion_len or (len(n_motion) >= 200):
+                                n_motion = motion[int(f_tag * 20) : int(to_tag * 20)]
+                                if (len(n_motion)) < min_motion_len or (
+                                    len(n_motion) >= 200
+                                ):
                                     continue
-                                new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
+                                new_name = (
+                                    random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                    + "_"
+                                    + name
+                                )
                                 while new_name in data_dict:
-                                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
+                                    new_name = (
+                                        random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                        + "_"
+                                        + name
+                                    )
                                 if self.num_frames != False:
                                     if len(n_motion) >= self.max_motion_length:
-                                        bias = random.randint(0, len(n_motion) - self.max_motion_length)
-                                        data_dict[new_name] = {'motion': n_motion[bias: bias+self.max_motion_length],
-                                                               'length': self.max_motion_length,
-                                                               'text': [text_dict]}
+                                        bias = random.randint(
+                                            0, len(n_motion) - self.max_motion_length
+                                        )
+                                        data_dict[new_name] = {
+                                            "motion": n_motion[
+                                                bias : bias + self.max_motion_length
+                                            ],
+                                            "length": self.max_motion_length,
+                                            "text": [text_dict],
+                                        }
                                         length_list.append(self.max_motion_length)
 
                                     else:
-                                        data_dict[new_name] = {'motion': n_motion,
-                                                               'length': len(n_motion),
-                                                               'text': [text_dict]}
+                                        data_dict[new_name] = {
+                                            "motion": n_motion,
+                                            "length": len(n_motion),
+                                            "text": [text_dict],
+                                        }
                                         length_list.append(len(n_motion))
 
                                 else:
-                                    data_dict[new_name] = {'motion': n_motion,
-                                                           'length': len(n_motion),
-                                                           'text':[text_dict]}
+                                    data_dict[new_name] = {
+                                        "motion": n_motion,
+                                        "length": len(n_motion),
+                                        "text": [text_dict],
+                                    }
                                     length_list.append(len(n_motion))
 
                                 new_name_list.append(new_name)
@@ -296,22 +349,30 @@ class Text2MotionDatasetV2(data.Dataset):
                 if flag:
                     if self.num_frames != False:
                         if len(motion) >= self.max_motion_length:
-                            bias = random.randint(0, len(motion) - self.max_motion_length)
-                            data_dict[name] = {'motion': motion[bias: bias + self.max_motion_length],
-                                                   'length': self.max_motion_length,
-                                                   'text': [text_dict]}
+                            bias = random.randint(
+                                0, len(motion) - self.max_motion_length
+                            )
+                            data_dict[name] = {
+                                "motion": motion[bias : bias + self.max_motion_length],
+                                "length": self.max_motion_length,
+                                "text": [text_dict],
+                            }
                             length_list.append(self.max_motion_length)
 
                         else:
-                            data_dict[name] = {'motion': motion,
-                                               'length': len(motion),
-                                               'text': text_data}
+                            data_dict[name] = {
+                                "motion": motion,
+                                "length": len(motion),
+                                "text": text_data,
+                            }
                             length_list.append(len(motion))
 
                     else:
-                        data_dict[name] = {'motion': motion,
-                                           'length': len(motion),
-                                           'text': text_data}
+                        data_dict[name] = {
+                            "motion": motion,
+                            "length": len(motion),
+                            "text": text_data,
+                        }
                         length_list.append(len(motion))
 
                     new_name_list.append(name)
@@ -319,7 +380,9 @@ class Text2MotionDatasetV2(data.Dataset):
                 print(e)
                 pass
 
-        name_list, length_list = zip(*sorted(zip(new_name_list, length_list), key=lambda x: x[1]))
+        name_list, length_list = zip(
+            *sorted(zip(new_name_list, length_list), key=lambda x: x[1])
+        )
 
         self.mean = mean
         self.std = std
@@ -331,7 +394,7 @@ class Text2MotionDatasetV2(data.Dataset):
     def reset_max_len(self, length):
         assert length <= self.max_motion_length
         self.pointer = np.searchsorted(self.length_arr, length)
-        print("Pointer Pointing at %d"%self.pointer)
+        logger.debug("Pointer Pointing at %d" % self.pointer)
         self.max_length = length
 
     def inv_transform(self, data):
@@ -343,20 +406,20 @@ class Text2MotionDatasetV2(data.Dataset):
     def __getitem__(self, item):
         idx = self.pointer + item
         data = self.data_dict[self.name_list[idx]]
-        motion, m_length, text_list = data['motion'], data['length'], data['text']
+        motion, m_length, text_list = data["motion"], data["length"], data["text"]
         # Randomly select a caption
         text_data = random.choice(text_list)
-        caption, tokens = text_data['caption'], text_data['tokens']
+        caption, tokens = text_data["caption"], text_data["tokens"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -369,32 +432,46 @@ class Text2MotionDatasetV2(data.Dataset):
 
         # Crop the motions in to times of 4, and introduce small variations
         if self.opt.unit_length < 10:
-            coin2 = np.random.choice(['single', 'single', 'double'])
+            coin2 = np.random.choice(["single", "single", "double"])
         else:
-            coin2 = 'single'
+            coin2 = "single"
 
-        if coin2 == 'double':
+        if coin2 == "double":
             m_length = (m_length // self.opt.unit_length - 1) * self.opt.unit_length
-        elif coin2 == 'single':
+        elif coin2 == "single":
             m_length = (m_length // self.opt.unit_length) * self.opt.unit_length
         idx = random.randint(0, len(motion) - m_length)
-        motion = motion[idx:idx+m_length]
+        motion = motion[idx : idx + m_length]
 
         "Z Normalization"
         motion = (motion - self.mean) / self.std
 
         if m_length < self.max_motion_length:
-            motion = np.concatenate([motion,
-                                     np.zeros((self.max_motion_length - m_length, motion.shape[1]))
-                                     ], axis=0)
+            motion = np.concatenate(
+                [
+                    motion,
+                    np.zeros((self.max_motion_length - m_length, motion.shape[1])),
+                ],
+                axis=0,
+            )
         # print(word_embeddings.shape, motion.shape)
         # print(tokens)
         # FIXME: I removed the extra return value ([]) at the end
-        return word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, '_'.join(tokens), []
+        return (
+            word_embeddings,
+            pos_one_hots,
+            caption,
+            sent_len,
+            motion,
+            m_length,
+            "_".join(tokens),
+            [],
+        )
+
 
+"""For use of training text motion matching model, and evaluations"""
 
 
-'''For use of training text motion matching model, and evaluations'''
 class PW3D_Text2MotionDatasetV2(data.Dataset):
     def __init__(self, opt, mean, std, splits, w_vectorizer, **kwargs):
         self.opt = opt
@@ -402,42 +479,74 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
         self.max_length = 20
         self.pointer = 0
         self.max_motion_length = opt.max_motion_length
-        self.min_motion_len = 80 if self.opt.dataset_name =='t2m' else 24
+        self.min_motion_len = 80 if self.opt.dataset_name == "t2m" else 24
         self.canon_relevant_entries = [0, 2, 6, 8]
 
         data_dict = {}
-        base_dir = './dataset/3dpw/new_joint_vecs'
-        splits = splits.split(',')
-        group_path = [[pjoin(base_dir, s, f) for f in os.listdir(pjoin(base_dir, s)) if (f.endswith('_p0.npy') or f.endswith('_p0_M.npy'))] for s in splits]
+        base_dir = "./dataset/3dpw/new_joint_vecs"
+        splits = splits.split(",")
+        group_path = [
+            [
+                pjoin(base_dir, s, f)
+                for f in os.listdir(pjoin(base_dir, s))
+                if (f.endswith("_p0.npy") or f.endswith("_p0_M.npy"))
+            ]
+            for s in splits
+        ]
         id_list = list(itertools.chain.from_iterable(group_path))
         new_name_list = []
         length_list = []
         for name in tqdm(id_list):
             name0 = name
-            name1 = name.replace('_p0', '_p1')
+            name1 = name.replace("_p0", "_p1")
             motion0 = np.load(name0)
             motion1 = np.load(name1)
+
             def get_canon(_name):
-                _canon = np.load(_name.replace('new_joint_vecs', 'canon_data'))[self.canon_relevant_entries] / 10.
-                return np.concatenate((_canon, np.zeros(self.opt.dim_pose-len(self.canon_relevant_entries))))
+                _canon = (
+                    np.load(_name.replace("new_joint_vecs", "canon_data"))[
+                        self.canon_relevant_entries
+                    ]
+                    / 10.0
+                )
+                return np.concatenate(
+                    (
+                        _canon,
+                        np.zeros(self.opt.dim_pose - len(self.canon_relevant_entries)),
+                    )
+                )
+
             canon0 = get_canon(name0)
             canon1 = get_canon(name1)
-            if not 'test' in splits:  # test is not yet annotated
-                with open(name0.replace('new_joint_vecs', 'text').replace('p0_M', 'p0').replace('.npy', '.txt'), 'r') as fr:
+            if "test" not in splits:  # test is not yet annotated
+                with open(
+                    name0.replace("new_joint_vecs", "text")
+                    .replace("p0_M", "p0")
+                    .replace(".npy", ".txt"),
+                    "r",
+                ) as fr:
                     texts = [t.strip() for t in fr.readlines()]
             else:
-                texts = [''] * 5
+                texts = [""] * 5
             assert len(texts) == 5
-            text_data = [{'caption': texts, 'tokens':['sos/OTHER', 'eos/OTHER']}]  # FIXME - parse tokens
-            new_name = os.path.basename(name).replace('.npy', '')
+            text_data = [
+                {"caption": texts, "tokens": ["sos/OTHER", "eos/OTHER"]}
+            ]  # FIXME - parse tokens
+            new_name = os.path.basename(name).replace(".npy", "")
             new_name_list.append(new_name)
             assert len(motion0) == len(motion1)
             length_list.append(len(motion0))
             # print('canon0', canon0)
             # print('canon1', canon1)
-            data_dict[new_name] = {'motion0': motion0, 'motion1': motion1,
-                               'length0': len(motion0), 'length1': len(motion1),
-                               'text': text_data, 'canon0': canon0, 'canon1': canon1,}
+            data_dict[new_name] = {
+                "motion0": motion0,
+                "motion1": motion1,
+                "length0": len(motion0),
+                "length1": len(motion1),
+                "text": text_data,
+                "canon0": canon0,
+                "canon1": canon1,
+            }
 
         # replicate data for beter utilization
         n_replications = 1000
@@ -445,15 +554,16 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
         rep_name_list = []
         rep_length_list = []
         for rep_i in range(n_replications):
-            rep_name_list += [e+'_{:04d}'.format(rep_i) for e in new_name_list]
+            rep_name_list += [e + "_{:04d}".format(rep_i) for e in new_name_list]
             rep_length_list += length_list
-            rep_data_dict.update({
-                k+'_{:04d}'.format(rep_i): v for k, v in data_dict.items()
-            })
-
+            rep_data_dict.update(
+                {k + "_{:04d}".format(rep_i): v for k, v in data_dict.items()}
+            )
 
         # name_list, length_list = zip(*sorted(zip(new_name_list, length_list), key=lambda x: x[1]))
-        name_list, length_list = zip(*sorted(zip(rep_name_list, rep_length_list), key=lambda x: x[1]))
+        name_list, length_list = zip(
+            *sorted(zip(rep_name_list, rep_length_list), key=lambda x: x[1])
+        )
 
         self.mean = mean
         self.std = std
@@ -465,24 +575,38 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
     def reset_max_len(self, length):
         assert length <= self.max_motion_length
         self.pointer = np.searchsorted(self.length_arr, length)
-        print("Pointer Pointing at %d"%self.pointer)
+        logger.debug("Pointer Pointing at %d" % self.pointer)
         self.max_length = length
 
     def rebuilt_canon(self, canon_pred):
         # asuumes canon data in the first 4 entries of the last dim
         # canon_pred *= 10.
-        canon_pred_scaled = canon_pred * 10.
+        canon_pred_scaled = canon_pred * 10.0
         first_entry = canon_pred_scaled[..., [0]]
         zeros = torch.zeros_like(first_entry)
         ones = torch.ones_like(first_entry)
-        return torch.cat((canon_pred_scaled[..., [0]], zeros, canon_pred_scaled[..., [1]], zeros, ones, zeros,
-                          canon_pred_scaled[..., [2]], zeros, canon_pred_scaled[..., [3]]), dim=-1)
+        return torch.cat(
+            (
+                canon_pred_scaled[..., [0]],
+                zeros,
+                canon_pred_scaled[..., [1]],
+                zeros,
+                ones,
+                zeros,
+                canon_pred_scaled[..., [2]],
+                zeros,
+                canon_pred_scaled[..., [3]],
+            ),
+            dim=-1,
+        )
 
     def inv_transform(self, data):
         return data * self.std + self.mean
 
     def inv_transform_torch(self, data):
-        return data * torch.tensor(self.std, dtype=data.dtype, device=data.device) + torch.tensor(self.mean, dtype=data.dtype, device=data.device)
+        return data * torch.tensor(
+            self.std, dtype=data.dtype, device=data.device
+        ) + torch.tensor(self.mean, dtype=data.dtype, device=data.device)
 
     def __len__(self):
         return len(self.data_dict) - self.pointer
@@ -490,34 +614,41 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
     def __getitem__(self, item):
         idx = self.pointer + item
         data = self.data_dict[self.name_list[idx]]
-        person_i = random.randint(0,1)
-        motion, m_length, text_list, canon = data[f'motion{person_i}'], data[f'length{person_i}'], data['text'], data[f'canon{person_i}']
-        other_motion = data[f'motion{1-person_i}']
-        other_canon = data[f'canon{1-person_i}']
+        person_i = random.randint(0, 1)
+        motion, m_length, text_list, canon = (
+            data[f"motion{person_i}"],
+            data[f"length{person_i}"],
+            data["text"],
+            data[f"canon{person_i}"],
+        )
+        other_motion = data[f"motion{1-person_i}"]
+        other_canon = data[f"canon{1-person_i}"]
 
         orig_length = m_length
-        if self.opt.load_mode == 'prefix':
+        if self.opt.load_mode == "prefix":
             m_length = 80
-        elif self.opt.load_mode == 'text':
-            m_length = random.randint(self.min_motion_len, min(self.max_motion_length, orig_length))
-        offset = random.randint(0, orig_length-m_length-1)
-        motion = motion[offset:offset+m_length]
-        other_motion = other_motion[offset:offset+m_length]
+        elif self.opt.load_mode == "text":
+            m_length = random.randint(
+                self.min_motion_len, min(self.max_motion_length, orig_length)
+            )
+        offset = random.randint(0, orig_length - m_length - 1)
+        motion = motion[offset : offset + m_length]
+        other_motion = other_motion[offset : offset + m_length]
 
         # Randomly select a caption
         assert len(text_list) == 1
         text_data = text_list[0]
-        caption, tokens = random.choice(text_data['caption']), text_data['tokens']
+        caption, tokens = random.choice(text_data["caption"]), text_data["tokens"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -528,34 +659,43 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
         pos_one_hots = np.concatenate(pos_one_hots, axis=0)
         word_embeddings = np.concatenate(word_embeddings, axis=0)
 
-        if self.opt.load_mode == 'text':
+        if self.opt.load_mode == "text":
             # Crop the motions in to times of 4, and introduce small variations
             if self.opt.unit_length < 10:
-                coin2 = np.random.choice(['single', 'single', 'double'])
+                coin2 = np.random.choice(["single", "single", "double"])
             else:
-                coin2 = 'single'
+                coin2 = "single"
 
-            if coin2 == 'double':
+            if coin2 == "double":
                 m_length = (m_length // self.opt.unit_length - 1) * self.opt.unit_length
-            elif coin2 == 'single':
+            elif coin2 == "single":
                 m_length = (m_length // self.opt.unit_length) * self.opt.unit_length
             idx = random.randint(0, len(motion) - m_length)
-            motion = motion[idx:idx+m_length]
-            other_motion = other_motion[idx:idx+m_length]
-
+            motion = motion[idx : idx + m_length]
+            other_motion = other_motion[idx : idx + m_length]
 
         "Z Normalization"
         motion = (motion - self.mean) / self.std
         other_motion = (other_motion - self.mean) / self.std
 
-        if self.opt.load_mode == 'text':
+        if self.opt.load_mode == "text":
             if m_length < self.max_motion_length:
-                motion = np.concatenate([motion,
-                                         np.zeros((self.max_motion_length - m_length, motion.shape[1]))
-                                         ], axis=0)
-                other_motion = np.concatenate([other_motion,
-                                         np.zeros((self.max_motion_length - m_length, other_motion.shape[1]))
-                                         ], axis=0)
+                motion = np.concatenate(
+                    [
+                        motion,
+                        np.zeros((self.max_motion_length - m_length, motion.shape[1])),
+                    ],
+                    axis=0,
+                )
+                other_motion = np.concatenate(
+                    [
+                        other_motion,
+                        np.zeros(
+                            (self.max_motion_length - m_length, other_motion.shape[1])
+                        ),
+                    ],
+                    axis=0,
+                )
 
         # print(word_embeddings.shape, motion.shape)
         # print(tokens)
@@ -564,18 +704,49 @@ class PW3D_Text2MotionDatasetV2(data.Dataset):
         # Concat canon data
         motion = np.concatenate((canon[None], motion), axis=0)
         other_motion = np.concatenate((other_canon[None], other_motion), axis=0)
-        m_length += 1 # since we added the cannon in the begining
+        m_length += 1  # since we added the cannon in the begining
+
+        return (
+            other_motion,
+            pos_one_hots,
+            caption,
+            person_i,
+            motion,
+            m_length,
+            "_".join(tokens),
+            [],
+        )
+
 
-        return other_motion, pos_one_hots, caption, person_i, motion, m_length, '_'.join(tokens), []
+"""For training BABEL text2motion evaluators"""
 
 
-'''For training BABEL text2motion evaluators'''
 class BABEL_Text2MotionDatasetV2(BABEL):
 
-    def __init__(self, split, datapath, transforms, opt, mean, std, w_vectorizer, sampler, mode, **kwargs):
-        BABEL.__init__(self, datapath=datapath, transforms=transforms, split=split, sampler=sampler,
-                       parse_tokens=True, mode=mode, short_db=kwargs.get('short_db', False),
-                       cropping_sampler=kwargs.get('cropping_sampler', False))  # tokens are needed for training
+    def __init__(
+        self,
+        split,
+        datapath,
+        transforms,
+        opt,
+        mean,
+        std,
+        w_vectorizer,
+        sampler,
+        mode,
+        **kwargs,
+    ):
+        BABEL.__init__(
+            self,
+            datapath=datapath,
+            transforms=transforms,
+            split=split,
+            sampler=sampler,
+            parse_tokens=True,
+            mode=mode,
+            short_db=kwargs.get("short_db", False),
+            cropping_sampler=kwargs.get("cropping_sampler", False),
+        )  # tokens are needed for training
         self.opt = opt
         self.w_vectorizer = w_vectorizer
         self.max_length = 20
@@ -589,25 +760,24 @@ class BABEL_Text2MotionDatasetV2(BABEL):
 
     def __getitem__(self, item):
         keyid = self._split_index[item]
-        batch = self.load_keyid(keyid, mode='train')
+        batch = self.load_keyid(keyid, mode="train")
 
         # Randomly choose a motion from batch
-        caption = batch['text']
-        tokens = batch['tokens']
-        motion = batch['features']
-        m_length = batch['length']
-        tansition_seq = batch['is_transition']
-
+        caption = batch["text"]
+        tokens = batch["tokens"]
+        motion = batch["features"]
+        m_length = batch["length"]
+        tansition_seq = batch["is_transition"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -620,36 +790,51 @@ class BABEL_Text2MotionDatasetV2(BABEL):
 
         # Crop the motions in to times of 4, and introduce small variations
         if self.opt.unit_length < 10:
-            coin2 = np.random.choice(['single', 'single', 'double'])
+            coin2 = np.random.choice(["single", "single", "double"])
         else:
-            coin2 = 'single'
+            coin2 = "single"
 
-        if coin2 == 'double':
+        if coin2 == "double":
             m_length = (m_length // self.opt.unit_length - 1) * self.opt.unit_length
-        elif coin2 == 'single':
+        elif coin2 == "single":
             m_length = (m_length // self.opt.unit_length) * self.opt.unit_length
 
         idx = random.randint(0, abs(len(motion) - m_length))
 
-        motion = motion[idx:idx+m_length]
-        tansition_seq = tansition_seq[idx:idx+m_length]
+        motion = motion[idx : idx + m_length]
+        tansition_seq = tansition_seq[idx : idx + m_length]
 
         "Z Normalization"
         motion = (motion - self.mean) / self.std
 
         if m_length <= self.max_motion_length:
-            motion = np.concatenate([motion,
-                                     np.zeros((self.max_motion_length - m_length, motion.shape[1]))
-                                     ], axis=0)
-            tansition_seq = np.concatenate([tansition_seq,
-                                     np.zeros(self.max_motion_length - m_length)
-                                     ])
+            motion = np.concatenate(
+                [
+                    motion,
+                    np.zeros((self.max_motion_length - m_length, motion.shape[1])),
+                ],
+                axis=0,
+            )
+            tansition_seq = np.concatenate(
+                [tansition_seq, np.zeros(self.max_motion_length - m_length)]
+            )
         # print(word_embeddings.shape, motion.shape)
         # print(tokens)
-        return word_embeddings, pos_one_hots, caption, sent_len, motion, m_length, '_'.join(tokens), tansition_seq
+        return (
+            word_embeddings,
+            pos_one_hots,
+            caption,
+            sent_len,
+            motion,
+            m_length,
+            "_".join(tokens),
+            tansition_seq,
+        )
+
+
+"""For use of training baseline"""
 
 
-'''For use of training baseline'''
 class Text2MotionDatasetBaseline(data.Dataset):
     def __init__(self, opt, mean, std, split_file, w_vectorizer):
         self.opt = opt
@@ -657,11 +842,11 @@ class Text2MotionDatasetBaseline(data.Dataset):
         self.max_length = 20
         self.pointer = 0
         self.max_motion_length = opt.max_motion_length
-        min_motion_len = 40 if self.opt.dataset_name =='t2m' else 24
+        min_motion_len = 40 if self.opt.dataset_name == "t2m" else 24
 
         data_dict = {}
         id_list = []
-        with cs.open(split_file, 'r') as f:
+        with cs.open(split_file, "r") as f:
             for line in f.readlines():
                 id_list.append(line.strip())
         # id_list = id_list[:200]
@@ -670,38 +855,50 @@ class Text2MotionDatasetBaseline(data.Dataset):
         length_list = []
         for name in tqdm(id_list):
             try:
-                motion = np.load(pjoin(opt.motion_dir, name + '.npy'))
+                motion = np.load(pjoin(opt.motion_dir, name + ".npy"))
                 if (len(motion)) < min_motion_len or (len(motion) >= 200):
                     continue
                 text_data = []
                 flag = False
-                with cs.open(pjoin(opt.text_dir, name + '.txt')) as f:
+                with cs.open(pjoin(opt.text_dir, name + ".txt")) as f:
                     for line in f.readlines():
                         text_dict = {}
-                        line_split = line.strip().split('#')
+                        line_split = line.strip().split("#")
                         caption = line_split[0]
-                        tokens = line_split[1].split(' ')
+                        tokens = line_split[1].split(" ")
                         f_tag = float(line_split[2])
                         to_tag = float(line_split[3])
                         f_tag = 0.0 if np.isnan(f_tag) else f_tag
                         to_tag = 0.0 if np.isnan(to_tag) else to_tag
 
-                        text_dict['caption'] = caption
-                        text_dict['tokens'] = tokens
+                        text_dict["caption"] = caption
+                        text_dict["tokens"] = tokens
                         if f_tag == 0.0 and to_tag == 0.0:
                             flag = True
                             text_data.append(text_dict)
                         else:
                             try:
-                                n_motion = motion[int(f_tag*20) : int(to_tag*20)]
-                                if (len(n_motion)) < min_motion_len or (len(n_motion) >= 200):
+                                n_motion = motion[int(f_tag * 20) : int(to_tag * 20)]
+                                if (len(n_motion)) < min_motion_len or (
+                                    len(n_motion) >= 200
+                                ):
                                     continue
-                                new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
+                                new_name = (
+                                    random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                    + "_"
+                                    + name
+                                )
                                 while new_name in data_dict:
-                                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
-                                data_dict[new_name] = {'motion': n_motion,
-                                                       'length': len(n_motion),
-                                                       'text':[text_dict]}
+                                    new_name = (
+                                        random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                        + "_"
+                                        + name
+                                    )
+                                data_dict[new_name] = {
+                                    "motion": n_motion,
+                                    "length": len(n_motion),
+                                    "text": [text_dict],
+                                }
                                 new_name_list.append(new_name)
                                 length_list.append(len(n_motion))
                             except:
@@ -710,15 +907,19 @@ class Text2MotionDatasetBaseline(data.Dataset):
                                 # break
 
                 if flag:
-                    data_dict[name] = {'motion': motion,
-                                       'length': len(motion),
-                                       'text': text_data}
+                    data_dict[name] = {
+                        "motion": motion,
+                        "length": len(motion),
+                        "text": text_data,
+                    }
                     new_name_list.append(name)
                     length_list.append(len(motion))
             except:
                 pass
 
-        name_list, length_list = zip(*sorted(zip(new_name_list, length_list), key=lambda x: x[1]))
+        name_list, length_list = zip(
+            *sorted(zip(new_name_list, length_list), key=lambda x: x[1])
+        )
 
         self.mean = mean
         self.std = std
@@ -730,7 +931,7 @@ class Text2MotionDatasetBaseline(data.Dataset):
     def reset_max_len(self, length):
         assert length <= self.max_motion_length
         self.pointer = np.searchsorted(self.length_arr, length)
-        print("Pointer Pointing at %d"%self.pointer)
+        logger.debug("Pointer Pointing at %d" % self.pointer)
         self.max_length = length
 
     def inv_transform(self, data):
@@ -742,20 +943,20 @@ class Text2MotionDatasetBaseline(data.Dataset):
     def __getitem__(self, item):
         idx = self.pointer + item
         data = self.data_dict[self.name_list[idx]]
-        motion, m_length, text_list = data['motion'], data['length'], data['text']
+        motion, m_length, text_list = data["motion"], data["length"], data["text"]
         # Randomly select a caption
         text_data = random.choice(text_list)
-        caption, tokens = text_data['caption'], text_data['tokens']
+        caption, tokens = text_data["caption"], text_data["tokens"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -771,14 +972,14 @@ class Text2MotionDatasetBaseline(data.Dataset):
         if m_length != self.max_length:
             # print("Motion original length:%d_%d"%(m_length, len(motion)))
             if self.opt.unit_length < 10:
-                coin2 = np.random.choice(['single', 'single', 'double'])
+                coin2 = np.random.choice(["single", "single", "double"])
             else:
-                coin2 = 'single'
-            if len_gap == 0 or (len_gap == 1 and coin2 == 'double'):
+                coin2 = "single"
+            if len_gap == 0 or (len_gap == 1 and coin2 == "double"):
                 m_length = self.max_length
                 s_idx = random.randint(0, m_length - self.max_length)
             else:
-                if coin2 == 'single':
+                if coin2 == "single":
                     n_m_length = self.max_length + self.opt.unit_length * len_gap
                 else:
                     n_m_length = self.max_length + self.opt.unit_length * (len_gap - 1)
@@ -787,17 +988,21 @@ class Text2MotionDatasetBaseline(data.Dataset):
         else:
             s_idx = 0
 
-        src_motion = motion[s_idx: s_idx + m_length]
-        tgt_motion = motion[s_idx: s_idx + self.max_length]
+        src_motion = motion[s_idx : s_idx + m_length]
+        tgt_motion = motion[s_idx : s_idx + self.max_length]
 
         "Z Normalization"
         src_motion = (src_motion - self.mean) / self.std
         tgt_motion = (tgt_motion - self.mean) / self.std
 
         if m_length < self.max_motion_length:
-            src_motion = np.concatenate([src_motion,
-                                     np.zeros((self.max_motion_length - m_length, motion.shape[1]))
-                                     ], axis=0)
+            src_motion = np.concatenate(
+                [
+                    src_motion,
+                    np.zeros((self.max_motion_length - m_length, motion.shape[1])),
+                ],
+                axis=0,
+            )
         # print(m_length, src_motion.shape, tgt_motion.shape)
         # print(word_embeddings.shape, motion.shape)
         # print(tokens)
@@ -812,13 +1017,13 @@ class MotionDatasetV2(data.Dataset):
         self.data = []
         self.lengths = []
         id_list = []
-        with cs.open(split_file, 'r') as f:
+        with cs.open(split_file, "r") as f:
             for line in f.readlines():
                 id_list.append(line.strip())
 
         for name in tqdm(id_list):
             try:
-                motion = np.load(pjoin(opt.motion_dir, name + '.npy'))
+                motion = np.load(pjoin(opt.motion_dir, name + ".npy"))
                 if motion.shape[0] < opt.window_size:
                     continue
                 self.lengths.append(motion.shape[0] - opt.window_size)
@@ -837,25 +1042,36 @@ class MotionDatasetV2(data.Dataset):
             # root_y (B, seq_len, 1)
             std[3:4] = std[3:4] / opt.feat_bias
             # ric_data (B, seq_len, (joint_num - 1)*3)
-            std[4: 4 + (joints_num - 1) * 3] = std[4: 4 + (joints_num - 1) * 3] / 1.0
+            std[4 : 4 + (joints_num - 1) * 3] = std[4 : 4 + (joints_num - 1) * 3] / 1.0
             # rot_data (B, seq_len, (joint_num - 1)*6)
-            std[4 + (joints_num - 1) * 3: 4 + (joints_num - 1) * 9] = std[4 + (joints_num - 1) * 3: 4 + (
-                        joints_num - 1) * 9] / 1.0
+            std[4 + (joints_num - 1) * 3 : 4 + (joints_num - 1) * 9] = (
+                std[4 + (joints_num - 1) * 3 : 4 + (joints_num - 1) * 9] / 1.0
+            )
             # local_velocity (B, seq_len, joint_num*3)
-            std[4 + (joints_num - 1) * 9: 4 + (joints_num - 1) * 9 + joints_num * 3] = std[
-                                                                                       4 + (joints_num - 1) * 9: 4 + (
-                                                                                                   joints_num - 1) * 9 + joints_num * 3] / 1.0
+            std[
+                4 + (joints_num - 1) * 9 : 4 + (joints_num - 1) * 9 + joints_num * 3
+            ] = (
+                std[
+                    4 + (joints_num - 1) * 9 : 4 + (joints_num - 1) * 9 + joints_num * 3
+                ]
+                / 1.0
+            )
             # foot contact (B, seq_len, 4)
-            std[4 + (joints_num - 1) * 9 + joints_num * 3:] = std[
-                                                              4 + (joints_num - 1) * 9 + joints_num * 3:] / opt.feat_bias
+            std[4 + (joints_num - 1) * 9 + joints_num * 3 :] = (
+                std[4 + (joints_num - 1) * 9 + joints_num * 3 :] / opt.feat_bias
+            )
 
             assert 4 + (joints_num - 1) * 9 + joints_num * 3 + 4 == mean.shape[-1]
-            np.save(pjoin(opt.meta_dir, 'mean.npy'), mean)
-            np.save(pjoin(opt.meta_dir, 'std.npy'), std)
+            np.save(pjoin(opt.meta_dir, "mean.npy"), mean)
+            np.save(pjoin(opt.meta_dir, "std.npy"), std)
 
         self.mean = mean
         self.std = std
-        print("Total number of motions {}, snippets {}".format(len(self.data), self.cumsum[-1]))
+        logger.info(
+            "Total number of motions {}, snippets {}".format(
+                len(self.data), self.cumsum[-1]
+            )
+        )
 
     def inv_transform(self, data):
         return data * self.std + self.mean
@@ -870,18 +1086,31 @@ class MotionDatasetV2(data.Dataset):
         else:
             motion_id = 0
             idx = 0
-        motion = self.data[motion_id][idx:idx+self.opt.window_size]
+        motion = self.data[motion_id][idx : idx + self.opt.window_size]
         "Z Normalization"
         motion = (motion - self.mean) / self.std
 
         return motion
 
-'''For training BABEL movement encoder (used by evaluators)'''
+
+"""For training BABEL movement encoder (used by evaluators)"""
+
+
 class BABEL_MotionDatasetV2(BABEL):
 
-    def __init__(self, split, datapath, transforms, opt, mean, std, sampler, mode, **kwargs):
-        BABEL.__init__(self, datapath=datapath, transforms=transforms, split=split,  sampler=sampler,
-                       parse_tokens=False, mode=mode, **kwargs)  # Tokens are not needed
+    def __init__(
+        self, split, datapath, transforms, opt, mean, std, sampler, mode, **kwargs
+    ):
+        BABEL.__init__(
+            self,
+            datapath=datapath,
+            transforms=transforms,
+            split=split,
+            sampler=sampler,
+            parse_tokens=False,
+            mode=mode,
+            **kwargs,
+        )  # Tokens are not needed
         self.opt = opt
         self.max_length = 20
         self.pointer = 0
@@ -895,40 +1124,42 @@ class BABEL_MotionDatasetV2(BABEL):
     def __getitem__(self, item):
 
         keyid = self._split_index[item]
-        batch = self.load_keyid(keyid, mode='train')
-        motion_type = MOTION_TYPES[random.randint(0, len(MOTION_TYPES)-1)]
-        motion = batch['features' + motion_type]
-        m_length = batch['length' + motion_type]
+        batch = self.load_keyid(keyid, mode="train")
+        motion_type = MOTION_TYPES[random.randint(0, len(MOTION_TYPES) - 1)]
+        motion = batch["features" + motion_type]
+        m_length = batch["length" + motion_type]
         assert m_length >= self.opt.window_size
 
         idx = random.randint(0, m_length - self.opt.window_size)
-        _motion = motion[idx:idx + self.opt.window_size]
+        _motion = motion[idx : idx + self.opt.window_size]
 
         "Z Normalization"
         _motion = (_motion - self.mean) / self.std
 
         return _motion
 
+
 class RawTextDataset(data.Dataset):
     def __init__(self, opt, mean, std, text_file, w_vectorizer):
         self.mean = mean
         self.std = std
         self.opt = opt
         self.data_dict = []
-        self.nlp = spacy.load('en_core_web_sm')
+        self.nlp = spacy.load("en_core_web_sm")
 
         with cs.open(text_file) as f:
             for line in f.readlines():
                 word_list, pos_list = self.process_text(line.strip())
-                tokens = ['%s/%s'%(word_list[i], pos_list[i]) for i in range(len(word_list))]
-                self.data_dict.append({'caption':line.strip(), "tokens":tokens})
+                tokens = [
+                    "%s/%s" % (word_list[i], pos_list[i]) for i in range(len(word_list))
+                ]
+                self.data_dict.append({"caption": line.strip(), "tokens": tokens})
 
         self.w_vectorizer = w_vectorizer
         print("Total number of descriptions {}".format(len(self.data_dict)))
 
-
     def process_text(self, sentence):
-        sentence = sentence.replace('-', '')
+        sentence = sentence.replace("-", "")
         doc = self.nlp(sentence)
         word_list = []
         pos_list = []
@@ -936,7 +1167,7 @@ class RawTextDataset(data.Dataset):
             word = token.text
             if not word.isalpha():
                 continue
-            if (token.pos_ == 'NOUN' or token.pos_ == 'VERB') and (word != 'left'):
+            if (token.pos_ == "NOUN" or token.pos_ == "VERB") and (word != "left"):
                 word_list.append(token.lemma_)
             else:
                 word_list.append(word)
@@ -951,17 +1182,17 @@ class RawTextDataset(data.Dataset):
 
     def __getitem__(self, item):
         data = self.data_dict[item]
-        caption, tokens = data['caption'], data['tokens']
+        caption, tokens = data["caption"], data["tokens"]
 
         if len(tokens) < self.opt.max_text_len:
             # pad with "unk"
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
-            tokens = tokens + ['unk/OTHER'] * (self.opt.max_text_len + 2 - sent_len)
+            tokens = tokens + ["unk/OTHER"] * (self.opt.max_text_len + 2 - sent_len)
         else:
             # crop
-            tokens = tokens[:self.opt.max_text_len]
-            tokens = ['sos/OTHER'] + tokens + ['eos/OTHER']
+            tokens = tokens[: self.opt.max_text_len]
+            tokens = ["sos/OTHER"] + tokens + ["eos/OTHER"]
             sent_len = len(tokens)
         pos_one_hots = []
         word_embeddings = []
@@ -974,6 +1205,7 @@ class RawTextDataset(data.Dataset):
 
         return word_embeddings, pos_one_hots, caption, sent_len
 
+
 class TextOnlyDataset(data.Dataset):
     def __init__(self, opt, mean, std, split_file, size=None, **kwargs):
         self.mean = mean
@@ -984,10 +1216,9 @@ class TextOnlyDataset(data.Dataset):
         self.pointer = 0
         self.fixed_length = 120
 
-
         data_dict = {}
         id_list = []
-        with cs.open(split_file, 'r') as f:
+        with cs.open(split_file, "r") as f:
             for line in f.readlines():
                 id_list.append(line.strip())
         id_list = id_list[:size]
@@ -998,28 +1229,36 @@ class TextOnlyDataset(data.Dataset):
             try:
                 text_data = []
                 flag = False
-                with cs.open(pjoin(opt.text_dir, name + '.txt')) as f:
+                with cs.open(pjoin(opt.text_dir, name + ".txt")) as f:
                     for line in f.readlines():
                         text_dict = {}
-                        line_split = line.strip().split('#')
+                        line_split = line.strip().split("#")
                         caption = line_split[0]
-                        tokens = line_split[1].split(' ')
+                        tokens = line_split[1].split(" ")
                         f_tag = float(line_split[2])
                         to_tag = float(line_split[3])
                         f_tag = 0.0 if np.isnan(f_tag) else f_tag
                         to_tag = 0.0 if np.isnan(to_tag) else to_tag
 
-                        text_dict['caption'] = caption
-                        text_dict['tokens'] = tokens
+                        text_dict["caption"] = caption
+                        text_dict["tokens"] = tokens
                         if f_tag == 0.0 and to_tag == 0.0:
                             flag = True
                             text_data.append(text_dict)
                         else:
                             try:
-                                new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
+                                new_name = (
+                                    random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                    + "_"
+                                    + name
+                                )
                                 while new_name in data_dict:
-                                    new_name = random.choice('ABCDEFGHIJKLMNOPQRSTUVW') + '_' + name
-                                data_dict[new_name] = {'text':[text_dict]}
+                                    new_name = (
+                                        random.choice("ABCDEFGHIJKLMNOPQRSTUVW")
+                                        + "_"
+                                        + name
+                                    )
+                                data_dict[new_name] = {"text": [text_dict]}
                                 new_name_list.append(new_name)
                             except:
                                 print(line_split)
@@ -1027,7 +1266,7 @@ class TextOnlyDataset(data.Dataset):
                                 # break
 
                 if flag:
-                    data_dict[name] = {'text': text_data}
+                    data_dict[name] = {"text": text_data}
                     new_name_list.append(name)
             except:
                 pass
@@ -1045,27 +1284,32 @@ class TextOnlyDataset(data.Dataset):
     def __getitem__(self, item):
         idx = self.pointer + item
         data = self.data_dict[self.name_list[idx]]
-        text_list = data['text']
+        text_list = data["text"]
 
         # Randomly select a caption
         text_data = random.choice(text_list)
-        caption, tokens = text_data['caption'], text_data['tokens']
+        caption, tokens = text_data["caption"], text_data["tokens"]
         return None, None, caption, None, np.array([0]), self.fixed_length, None
         # fixed_length can be set from outside before sampling
 
+
 # A wrapper class for t2m original dataset for MDM purposes
 class HumanML3D(data.Dataset):
-    def __init__(self, load_mode, datapath='./dataset/humanml_opt.txt', split="train", **kwargs):
+    def __init__(
+        self, load_mode, datapath="./dataset/humanml_opt.txt", split="train", **kwargs
+    ):
         self.load_mode = load_mode
-        
-        self.dataset_name = 't2m'
-        self.dataname = 't2m'
+
+        self.dataset_name = "t2m"
+        self.dataname = "t2m"
         self.split = split
 
         # Configurations of T2M dataset and KIT dataset is almost the same
-        abs_base_path = f'.'
+        abs_base_path = pjoin(dirname(__file__), "../../..")
         dataset_opt_path = pjoin(abs_base_path, datapath)
-        device = None  # torch.device('cuda:4') # This param is not in use in this context
+        device = (
+            None  # torch.device('cuda:4') # This param is not in use in this context
+        )
         opt = get_opt(dataset_opt_path, device)
         opt.meta_dir = pjoin(abs_base_path, opt.meta_dir)
         opt.motion_dir = pjoin(abs_base_path, opt.motion_dir)
@@ -1074,43 +1318,59 @@ class HumanML3D(data.Dataset):
         opt.checkpoints_dir = pjoin(abs_base_path, opt.checkpoints_dir)
         opt.data_root = pjoin(abs_base_path, opt.data_root)
         opt.save_root = pjoin(abs_base_path, opt.save_root)
-        opt.meta_dir = './dataset'
+        opt.meta_dir = "./dataset"
         opt.load_mode = load_mode
         self.opt = opt
-        print('Loading dataset %s ...' % opt.dataset_name)
+        logger.info("Loading dataset %s ..." % opt.dataset_name)
 
-        if load_mode == 'gt':
+        if load_mode == "gt":
             # used by T2M models (including evaluators)
-            self.mean = np.load(pjoin(opt.meta_dir, f'{opt.dataset_name}_mean.npy'))
-            self.std = np.load(pjoin(opt.meta_dir, f'{opt.dataset_name}_std.npy'))
-        elif load_mode in ['train', 'eval', 'text_only', 'prefix', 'text']:
+            self.mean = np.load(pjoin(opt.meta_dir, f"{opt.dataset_name}_mean.npy"))
+            self.std = np.load(pjoin(opt.meta_dir, f"{opt.dataset_name}_std.npy"))
+        elif load_mode in ["train", "eval", "text_only", "prefix", "text"]:
             # used by our models
-            self.mean = np.load(pjoin(opt.data_root, 'Mean.npy'))
-            self.std = np.load(pjoin(opt.data_root, 'Std.npy'))
+            self.mean = np.load(pjoin(opt.data_root, "Mean.npy"))
+            self.std = np.load(pjoin(opt.data_root, "Std.npy"))
 
-        if load_mode == 'eval':
+        if load_mode == "eval":
             # used by T2M models (including evaluators)
             # this is to translate their norms to ours
-            self.mean_for_eval = np.load(pjoin(opt.meta_dir, f'{opt.dataset_name}_mean.npy'))
-            self.std_for_eval = np.load(pjoin(opt.meta_dir, f'{opt.dataset_name}_std.npy'))
-
-        self.split_file = pjoin(opt.data_root, f'{split}.txt')
-        if load_mode == 'text_only':
-            self.t2m_dataset = TextOnlyDataset(self.opt, self.mean, self.std, self.split_file, **kwargs)
+            self.mean_for_eval = np.load(
+                pjoin(opt.meta_dir, f"{opt.dataset_name}_mean.npy")
+            )
+            self.std_for_eval = np.load(
+                pjoin(opt.meta_dir, f"{opt.dataset_name}_std.npy")
+            )
+
+        self.split_file = pjoin(opt.data_root, f"{split}.txt")
+        if load_mode == "text_only":
+            self.t2m_dataset = TextOnlyDataset(
+                self.opt, self.mean, self.std, self.split_file, **kwargs
+            )
         else:
-            self.w_vectorizer = WordVectorizer(pjoin(abs_base_path, 'glove'), 'our_vab')
+            self.w_vectorizer = WordVectorizer(pjoin(abs_base_path, "glove"), "our_vab")
 
-            if hasattr(opt, 'dataset_type') and opt.dataset_type == 'pw3d':
-                self.t2m_dataset = PW3D_Text2MotionDatasetV2(self.opt, self.mean, self.std, self.split,
-                                                             self.w_vectorizer)
+            if hasattr(opt, "dataset_type") and opt.dataset_type == "pw3d":
+                self.t2m_dataset = PW3D_Text2MotionDatasetV2(
+                    self.opt, self.mean, self.std, self.split, self.w_vectorizer
+                )
             else:
-                self.t2m_dataset = Text2MotionDatasetV2(self.opt, self.mean, self.std, self.split_file, self.w_vectorizer, **kwargs)
-            self.num_actions = 1 # dummy placeholder
-
-        assert len(self.t2m_dataset) > 1, 'You loaded an empty dataset, ' \
-                                          'it is probably because your data dir has only texts and no motions.\n' \
-                                          'To train and evaluate MDM you should get the FULL data as described ' \
-                                          'in the README file.'
+                self.t2m_dataset = Text2MotionDatasetV2(
+                    self.opt,
+                    self.mean,
+                    self.std,
+                    self.split_file,
+                    self.w_vectorizer,
+                    **kwargs,
+                )
+            self.num_actions = 1  # dummy placeholder
+
+        assert len(self.t2m_dataset) > 1, (
+            "You loaded an empty dataset, "
+            "it is probably because your data dir has only texts and no motions.\n"
+            "To train and evaluate MDM you should get the FULL data as described "
+            "in the README file."
+        )
 
     def __getitem__(self, item):
         return self.t2m_dataset.__getitem__(item)
@@ -1118,80 +1378,112 @@ class HumanML3D(data.Dataset):
     def __len__(self):
         return self.t2m_dataset.__len__()
 
+
 # A wrapper class for t2m original dataset for MDM purposes
 class KIT(HumanML3D):
-    def __init__(self, load_mode, datapath='./dataset/kit_opt.txt', split="train", **kwargs):
+    def __init__(
+        self, load_mode, datapath="./dataset/kit_opt.txt", split="train", **kwargs
+    ):
         super(KIT, self).__init__(load_mode, datapath, split, **kwargs)
 
+
 # A wrapper class for t2m original dataset for MDM purposes
 class PW3D(HumanML3D):
-    def __init__(self, load_mode, datapath='./dataset/pw3d_opt.txt', split="train", **kwargs):
+    def __init__(
+        self, load_mode, datapath="./dataset/pw3d_opt.txt", split="train", **kwargs
+    ):
         super(PW3D, self).__init__(load_mode, datapath, split, **kwargs)
 
 
 # A wrapper class for t2m original dataset for MDM purposes
 class BABEL_eval(data.Dataset):
-    def __init__(self, load_mode, datapath, transforms, sampler, mode, opt, split="train", **kwargs):
+    def __init__(
+        self,
+        load_mode,
+        datapath,
+        transforms,
+        sampler,
+        mode,
+        opt,
+        split="train",
+        **kwargs,
+    ):
         self.load_mode = load_mode
 
         self.split = split
         self.datapath = datapath
-        abs_base_path = f'.'
+        abs_base_path = "."
 
         if opt is None:
-            self.opt_path = './dataset/humanml_opt.txt'
+            self.opt_path = "./dataset/humanml_opt.txt"
             # Configurations of T2M dataset and KIT dataset is almost the same
             dataset_opt_path = pjoin(abs_base_path, self.opt_path)
             device = None  # torch.device('cuda:4') # This param is not in use in this context
             opt = get_opt(dataset_opt_path, device)
-            opt.data_root = pjoin('dataset', 'babel')
+            opt.data_root = pjoin("dataset", "babel")
             opt.meta_dir = pjoin(abs_base_path, opt.meta_dir)
             opt.motion_dir = pjoin(abs_base_path, opt.motion_dir)
             opt.text_dir = pjoin(abs_base_path, opt.text_dir)
             opt.model_dir = None
-            opt.checkpoints_dir = '.'
+            opt.checkpoints_dir = "."
             opt.data_root = pjoin(abs_base_path, opt.data_root)
             opt.save_root = pjoin(abs_base_path, opt.save_root)
-            opt.meta_dir = './dataset'
+            opt.meta_dir = "./dataset"
             opt.dim_pose = 135
             opt.foot_contact_entries = 0
-            opt.dataset_name = 'babel'
-            opt.decomp_name = 'Decomp_SP001_SM001_H512_babel_2700epoch'
-            opt.meta_root = pjoin(opt.checkpoints_dir, opt.dataset_name, 'motion1', 'meta')
-            opt.min_motion_length = sampler.min_len # must be at least window size
+            opt.dataset_name = "babel"
+            opt.decomp_name = "Decomp_SP001_SM001_H512_babel_2700epoch"
+            opt.meta_root = pjoin(
+                opt.checkpoints_dir, opt.dataset_name, "motion1", "meta"
+            )
+            opt.min_motion_length = sampler.min_len  # must be at least window size
             opt.max_motion_length = sampler.max_len
         self.opt = opt
 
-        print('Loading dataset %s ...' % opt.dataset_name)
+        logger.info("Loading dataset %s ..." % opt.dataset_name)
 
         self.dataset_name = opt.dataset_name
         self.dataname = opt.dataset_name
         self.sampler = sampler
         self.transforms = transforms
-        self.mean = np.zeros([opt.dim_pose], dtype=np.float32)  # data is already normalized
-        self.std = np.ones([opt.dim_pose], dtype=np.float32)  # data is already normalized
-
-        DATA = BABEL_MotionDatasetV2 if load_mode == 'movement_train' else BABEL_Text2MotionDatasetV2
+        self.mean = np.zeros(
+            [opt.dim_pose], dtype=np.float32
+        )  # data is already normalized
+        self.std = np.ones(
+            [opt.dim_pose], dtype=np.float32
+        )  # data is already normalized
+
+        DATA = (
+            BABEL_MotionDatasetV2
+            if load_mode == "movement_train"
+            else BABEL_Text2MotionDatasetV2
+        )
 
-        self.w_vectorizer = WordVectorizer(pjoin(abs_base_path, 'glove'), 'our_vab')
+        self.w_vectorizer = WordVectorizer(pjoin(abs_base_path, "glove"), "our_vab")
         self.t2m_dataset = DATA(
             split=self.split,
             datapath=self.datapath,
             transforms=self.transforms,
             mode=mode,
             opt=self.opt,
-            mean=self.mean, std=self.std, w_vectorizer=self.w_vectorizer, sampler=self.sampler,
-            short_db=kwargs.get('short_db', False), cropping_sampler=kwargs.get('cropping_sampler', False)
+            mean=self.mean,
+            std=self.std,
+            w_vectorizer=self.w_vectorizer,
+            sampler=self.sampler,
+            short_db=kwargs.get("short_db", False),
+            cropping_sampler=kwargs.get("cropping_sampler", False),
         )
         self.num_actions = 1  # dummy placeholder
 
-        assert len(self.t2m_dataset) > 1, 'You loaded an empty dataset, ' \
-                                          'it is probably because your data dir has only texts and no motions.\n' \
-                                          'To train and evaluate MDM you should get the FULL data as described ' \
-                                          'in the README file.'
+        assert len(self.t2m_dataset) > 1, (
+            "You loaded an empty dataset, "
+            "it is probably because your data dir has only texts and no motions.\n"
+            "To train and evaluate MDM you should get the FULL data as described "
+            "in the README file."
+        )
 
     def __getitem__(self, item):
         return self.t2m_dataset.__getitem__(item)
 
     def __len__(self):
-        return self.t2m_dataset.__len__()
\ No newline at end of file
+        return self.t2m_dataset.__len__()
diff --git a/data_loaders/humanml/motion_loaders/comp_v6_model_dataset.py b/data_loaders/humanml/motion_loaders/comp_v6_model_dataset.py
index ee77764..ab8cda5 100644
--- a/data_loaders/humanml/motion_loaders/comp_v6_model_dataset.py
+++ b/data_loaders/humanml/motion_loaders/comp_v6_model_dataset.py
@@ -1,14 +1,14 @@
 import itertools
 
 import torch
-from data_loaders.humanml.networks.modules import *
-from data_loaders.humanml.networks.trainers import CompTrainerV6
+from ..networks.modules import *
+from ..networks.trainers import CompTrainerV6
 from torch.utils.data import Dataset, DataLoader
 from os.path import join as pjoin
 from tqdm import tqdm
-from data_loaders.humanml_utils import get_inpainting_mask
+from ...humanml_utils import get_inpainting_mask
 
-from utils.sampling_utils import double_take_arb_len, unfold_sample_arb_len
+from ....utils.sampling_utils import double_take_arb_len, unfold_sample_arb_len
 from utils import dist_util
 
 def build_models(opt):
diff --git a/data_loaders/humanml/motion_loaders/model_motion_loaders.py b/data_loaders/humanml/motion_loaders/model_motion_loaders.py
index 68aa2e7..ca7479d 100644
--- a/data_loaders/humanml/motion_loaders/model_motion_loaders.py
+++ b/data_loaders/humanml/motion_loaders/model_motion_loaders.py
@@ -1,8 +1,8 @@
 from torch.utils.data import DataLoader, Dataset
-from data_loaders.humanml.utils.get_opt import get_opt
-from data_loaders.humanml.motion_loaders.comp_v6_model_dataset import CompMDMGeneratedDataset, CompMDMInpaintingGeneratedDataset, \
+from ..utils.get_opt import get_opt
+from .comp_v6_model_dataset import CompMDMGeneratedDataset, CompMDMInpaintingGeneratedDataset, \
     CompMDMUnfoldingGeneratedDataset
-from data_loaders.humanml.utils.word_vectorizer import WordVectorizer
+from ..utils.word_vectorizer import WordVectorizer
 import numpy as np
 from torch.utils.data._utils.collate import default_collate
 
diff --git a/data_loaders/humanml/networks/evaluator_wrapper.py b/data_loaders/humanml/networks/evaluator_wrapper.py
index 8a49541..01ed151 100644
--- a/data_loaders/humanml/networks/evaluator_wrapper.py
+++ b/data_loaders/humanml/networks/evaluator_wrapper.py
@@ -1,5 +1,5 @@
-from data_loaders.humanml.networks.modules import *
-from data_loaders.humanml.utils.word_vectorizer import POS_enumerator
+from .modules import *
+from ..utils.word_vectorizer import POS_enumerator
 from os.path import join as pjoin
 
 
diff --git a/data_loaders/humanml/networks/trainers.py b/data_loaders/humanml/networks/trainers.py
index 663a207..10b8dc1 100644
--- a/data_loaders/humanml/networks/trainers.py
+++ b/data_loaders/humanml/networks/trainers.py
@@ -1,17 +1,17 @@
 import torch
 import torch.nn.functional as F
 import random
-from data_loaders.humanml.networks.modules import *
+from .modules import *
 from torch.utils.data import DataLoader
 import torch.optim as optim
 from torch.nn.utils import clip_grad_norm_
 # import tensorflow as tf
 from collections import OrderedDict
-from data_loaders.humanml.utils.utils import *
+from ..utils.utils import *
 from os.path import join as pjoin
-from data_loaders.humanml.data.dataset import collate_fn
+from ..data.dataset import collate_fn
 import codecs as cs
-from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
+from ....train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
 
 
 class Logger(object):
diff --git a/data_loaders/humanml/options/train_options.py b/data_loaders/humanml/options/train_options.py
index c2915dd..684f0d4 100644
--- a/data_loaders/humanml/options/train_options.py
+++ b/data_loaders/humanml/options/train_options.py
@@ -1,4 +1,4 @@
-from data_loaders.humanml.options.base_options import BaseOptions
+from .base_options import BaseOptions
 import argparse
 
 class TrainCompOptions(BaseOptions):
diff --git a/data_loaders/humanml/scripts/motion_process.py b/data_loaders/humanml/scripts/motion_process.py
index 704ce20..9df5175 100644
--- a/data_loaders/humanml/scripts/motion_process.py
+++ b/data_loaders/humanml/scripts/motion_process.py
@@ -1,10 +1,10 @@
 from os.path import join as pjoin
 
-from data_loaders.humanml.common.skeleton import Skeleton
+from ..common.skeleton import Skeleton
 import numpy as np
 import os
-from data_loaders.humanml.common.quaternion import *
-from data_loaders.humanml.utils.paramUtil import *
+from ..common.quaternion import *
+from ..utils.paramUtil import *
 
 import torch
 from tqdm import tqdm
diff --git a/data_loaders/humanml/train_decomp_v3.py b/data_loaders/humanml/train_decomp_v3.py
index 35932c9..6215799 100644
--- a/data_loaders/humanml/train_decomp_v3.py
+++ b/data_loaders/humanml/train_decomp_v3.py
@@ -6,17 +6,17 @@ from os.path import join as pjoin
 import data_loaders.humanml.utils.paramUtil as paramUtil
 from data_loaders.amass.sampling import FrameSampler
 from data_loaders.amass.transforms import SMPLTransform
-from data_loaders.get_data import get_dataset_loader
+from ..get_data import get_dataset_loader
 from data_loaders.humanml import collect_babel_stats
-from data_loaders.humanml.options.train_options import TrainDecompOptions
-from data_loaders.humanml.utils.plot_script import *
+from .options.train_options import TrainDecompOptions
+from .utils.plot_script import *
 
-from data_loaders.humanml.networks.modules import *
-from data_loaders.humanml.networks.trainers import DecompTrainerV3
-from data_loaders.humanml.data.dataset import MotionDatasetV2, BABEL_MotionDatasetV2
-from data_loaders.humanml.scripts.motion_process import *
+from .networks.modules import *
+from .networks.trainers import DecompTrainerV3
+from .data.dataset import MotionDatasetV2, BABEL_MotionDatasetV2
+from .scripts.motion_process import *
 from torch.utils.data import DataLoader
-from data_loaders.humanml.utils.word_vectorizer import WordVectorizer, POS_enumerator
+from .utils.word_vectorizer import WordVectorizer, POS_enumerator
 
 
 def plot_t2m(data, save_dir):
diff --git a/data_loaders/humanml/train_tex_mot_match.py b/data_loaders/humanml/train_tex_mot_match.py
index 88436a2..ea707fe 100644
--- a/data_loaders/humanml/train_tex_mot_match.py
+++ b/data_loaders/humanml/train_tex_mot_match.py
@@ -5,15 +5,15 @@ import torch
 
 from data_loaders.amass.sampling import FrameSampler
 from data_loaders.amass.transforms import SMPLTransform
-from data_loaders.get_data import get_dataset_loader
-from data_loaders.humanml.options.train_options import TrainTexMotMatchOptions
+from ..get_data import get_dataset_loader
+from .options.train_options import TrainTexMotMatchOptions
 
-from data_loaders.humanml.networks.modules import *
-from data_loaders.humanml.networks.trainers import TextMotionMatchTrainer
-from data_loaders.humanml.data.dataset import Text2MotionDatasetV2, collate_fn, BABEL_Text2MotionDatasetV2
-from data_loaders.humanml.scripts.motion_process import *
+from .networks.modules import *
+from .networks.trainers import TextMotionMatchTrainer
+from .data.dataset import Text2MotionDatasetV2, collate_fn, BABEL_Text2MotionDatasetV2
+from .scripts.motion_process import *
 from torch.utils.data import DataLoader
-from data_loaders.humanml.utils.word_vectorizer import WordVectorizer, POS_enumerator
+from .utils.word_vectorizer import WordVectorizer, POS_enumerator
 from copy import deepcopy
 from  data_loaders.humanml import collect_babel_stats
 
diff --git a/data_loaders/humanml/utils/get_opt.py b/data_loaders/humanml/utils/get_opt.py
index c8bb6dc..43537bc 100644
--- a/data_loaders/humanml/utils/get_opt.py
+++ b/data_loaders/humanml/utils/get_opt.py
@@ -1,15 +1,17 @@
-import os
 from argparse import Namespace
 import re
 from os.path import join as pjoin
-from data_loaders.humanml.utils.word_vectorizer import POS_enumerator
+from .word_vectorizer import POS_enumerator
+import logging
+
+logger = logging.getLogger(__name__)
 
 
 def is_float(numStr):
     flag = False
-    numStr = str(numStr).strip().lstrip('-').lstrip('+')    # (+)(-)
+    numStr = str(numStr).strip().lstrip("-").lstrip("+")  # (+)(-)
     try:
-        reg = re.compile(r'^[-+]?[0-9]+\.[0-9]+$')
+        reg = re.compile(r"^[-+]?[0-9]+\.[0-9]+$")
         res = reg.match(str(numStr))
         if res:
             flag = True
@@ -20,7 +22,7 @@ def is_float(numStr):
 
 def is_number(numStr):
     flag = False
-    numStr = str(numStr).strip().lstrip('-').lstrip('+')    # (+)(-)
+    numStr = str(numStr).strip().lstrip("-").lstrip("+")  # (+)(-)
     if str(numStr).isdigit():
         flag = True
     return flag
@@ -30,18 +32,20 @@ def get_opt(opt_path, device):
     opt = Namespace()
     opt_dict = vars(opt)
 
-    skip = ('-------------- End ----------------',
-            '------------ Options -------------',
-            '\n')
-    print('Reading', opt_path)
+    skip = (
+        "-------------- End ----------------",
+        "------------ Options -------------",
+        "\n",
+    )
+    logger.info(f"Reading {opt_path}")
     with open(opt_path) as f:
         for line in f:
             if line.strip() not in skip:
                 # print(line.strip())
-                key, value = line.strip().split(': ')
-                if value in ('True', 'False'):
+                key, value = line.strip().split(": ")
+                if value in ("True", "False"):
                     # opt_dict[key] = bool(value)
-                    opt_dict[key] = (value == 'True')
+                    opt_dict[key] = value == "True"
                 elif is_float(value):
                     opt_dict[key] = float(value)
                 elif is_number(value):
@@ -50,20 +54,20 @@ def get_opt(opt_path, device):
                     opt_dict[key] = str(value)
 
     # print(opt)
-    opt_dict['which_epoch'] = 'latest'
+    opt_dict["which_epoch"] = "latest"
     opt.save_root = pjoin(opt.checkpoints_dir, opt.dataset_name, opt.name)
-    opt.model_dir = pjoin(opt.save_root, 'model')
-    opt.meta_dir = pjoin(opt.save_root, 'meta')
+    opt.model_dir = pjoin(opt.save_root, "model")
+    opt.meta_dir = pjoin(opt.save_root, "meta")
 
-    if opt.dataset_name == 't2m':
-        opt.data_root = './dataset/HumanML3D'
-        opt.motion_dir = pjoin(opt.data_root, 'new_joint_vecs')
-        opt.text_dir = pjoin(opt.data_root, 'texts')
+    if opt.dataset_name == "t2m":
+        opt.data_root = "./dataset/HumanML3D"
+        opt.motion_dir = pjoin(opt.data_root, "new_joint_vecs")
+        opt.text_dir = pjoin(opt.data_root, "texts")
         opt.joints_num = 22
         opt.dim_pose = 263
         opt.max_motion_length = 196
     else:
-        raise KeyError('Dataset not recognized')
+        raise KeyError("Dataset not recognized")
 
     opt.dim_word = 300
     opt.num_classes = 200 // opt.unit_length
@@ -72,4 +76,4 @@ def get_opt(opt_path, device):
     opt.is_continue = False
     opt.device = device
 
-    return opt
\ No newline at end of file
+    return opt
diff --git a/data_loaders/tensors.py b/data_loaders/tensors.py
index 4bc47c6..9f42e78 100644
--- a/data_loaders/tensors.py
+++ b/data_loaders/tensors.py
@@ -1,12 +1,10 @@
-import random
 import torch
-from data_loaders.amass.tools import collate_tensor_with_padding
 
 def lengths_to_mask(lengths, max_len):
     # max_len = max(lengths)
     mask = torch.arange(max_len, device=lengths.device).expand(len(lengths), max_len) < lengths.unsqueeze(1)
     return mask
-    
+
 
 def collate_tensors(batch):
     dims = batch[0].dim()
diff --git a/dataset/pw3d_opt.txt b/dataset/pw3d_opt.txt
old mode 100755
new mode 100644
diff --git a/dataset/pw3d_prefix_opt.txt b/dataset/pw3d_prefix_opt.txt
old mode 100755
new mode 100644
diff --git a/diffusion/gaussian_diffusion.py b/diffusion/gaussian_diffusion.py
index 929f25c..655fc88 100644
--- a/diffusion/gaussian_diffusion.py
+++ b/diffusion/gaussian_diffusion.py
@@ -14,10 +14,10 @@ import torch
 import torch as th
 from copy import deepcopy
 
-from data_loaders.amass.transforms.smpl import SlimSMPLTransform
-from diffusion.nn import mean_flat, sum_flat
-from diffusion.losses import normal_kl, discretized_gaussian_log_likelihood
-from data_loaders.humanml.scripts import motion_process
+from priorMDM.data_loaders.amass.transforms.smpl import SlimSMPLTransform
+from .nn import mean_flat, sum_flat
+from .losses import normal_kl, discretized_gaussian_log_likelihood
+from priorMDM.data_loaders.humanml.scripts import motion_process
 
 def get_named_beta_schedule(schedule_name, num_diffusion_timesteps, scale_betas=1.):
     """
@@ -327,7 +327,7 @@ class GaussianDiffusion:
             inpainting_mask, inpainted_motion = model_kwargs['y']['inpainting_mask'], model_kwargs['y']['inpainted_motion']
             assert self.model_mean_type == ModelMeanType.START_X, 'This feature supports only X_start pred for mow!'
             assert model_output.shape == inpainting_mask.shape == inpainted_motion.shape
-            
+
             # inpainting_mask supports both boolean and scalar values
             ones = torch.ones_like(inpainting_mask, dtype=torch.float, device=inpainting_mask.device)
             inpainting_mask = ones * inpainting_mask
@@ -687,10 +687,7 @@ class GaussianDiffusion:
                 for sample_i, len in zip(range(1, sample['sample'].shape[0]), model_kwargs['y']['lengths']):
                     _suffix = sample['sample'][sample_i - 1, :, :, -unfolding_handshake + len:len]
                     _prefix = sample['sample'][sample_i, :, :, :unfolding_handshake]
-                    try:
-                        _blend = (_suffix * (1 - alpha) + _prefix * alpha)
-                    except(RuntimeError):
-                        print("Error")
+                    _blend = (_suffix * (1 - alpha) + _prefix * alpha)
                     sample['sample'][sample_i - 1, :, :, -unfolding_handshake + len:len] = _blend
                     sample['sample'][sample_i, :, :, :unfolding_handshake] = _blend
             elif ((unfolding_handshake > 0) and not (second_take_only)):
diff --git a/diffusion/inpainting_gaussian_diffusion.py b/diffusion/inpainting_gaussian_diffusion.py
index ab14dad..fd97bf0 100644
--- a/diffusion/inpainting_gaussian_diffusion.py
+++ b/diffusion/inpainting_gaussian_diffusion.py
@@ -1,4 +1,4 @@
-from diffusion.respace import SpacedDiffusion
+from .respace import SpacedDiffusion
 from .gaussian_diffusion import _extract_into_tensor
 import torch as th
 
diff --git a/eval/eval_babel.py b/eval/eval_babel.py
index aa10d4f..7df3521 100644
--- a/eval/eval_babel.py
+++ b/eval/eval_babel.py
@@ -1,20 +1,20 @@
-from model.DoubleTake_MDM import doubleTake_MDM
-from model.mdm import MDM
-from utils.parser_util import evaluation_double_take_parser
-from utils.fixseed import fixseed
+from ..model.DoubleTake_MDM import doubleTake_MDM
+from ..model.mdm import MDM
+from ..utils.parser_util import evaluation_double_take_parser
+from ..utils.fixseed import fixseed
 from datetime import datetime
-from data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
-from data_loaders.humanml.utils.metrics import *
-from data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
+from ..data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
+from ..data_loaders.humanml.utils.metrics import *
+from ..data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
 from collections import OrderedDict
-from data_loaders.humanml.scripts.motion_process import *
-from data_loaders.humanml.utils.utils import *
-from utils.model_util import load_model
+from ..data_loaders.humanml.scripts.motion_process import *
+from ..data_loaders.humanml.utils.utils import *
+from ..utils.model_util import load_model
 
 from diffusion import logger
 from utils import dist_util
-from data_loaders.get_data import get_dataset_loader
-from model.cfg_sampler import ClassifierFreeSampleModel
+from ..data_loaders.get_data import get_dataset_loader
+from ..model.cfg_sampler import ClassifierFreeSampleModel
 from copy import deepcopy
 
 torch.multiprocessing.set_sharing_strategy('file_system')
diff --git a/eval/eval_finetuned_motion_control.py b/eval/eval_finetuned_motion_control.py
index fe87573..95e51d8 100644
--- a/eval/eval_finetuned_motion_control.py
+++ b/eval/eval_finetuned_motion_control.py
@@ -1,20 +1,20 @@
-from diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
-from diffusion.respace import SpacedDiffusion
-from utils.parser_util import evaluation_inpainting_parser
-from utils.fixseed import fixseed
+from ..diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
+from ..diffusion.respace import SpacedDiffusion
+from ..utils.parser_util import evaluation_inpainting_parser
+from ..utils.fixseed import fixseed
 from datetime import datetime
-from data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
-from data_loaders.humanml.utils.metrics import *
-from data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
+from ..data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
+from ..data_loaders.humanml.utils.metrics import *
+from ..data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
 from collections import OrderedDict
-from data_loaders.humanml.scripts.motion_process import *
-from data_loaders.humanml.utils.utils import *
-from utils.model_util import load_model_blending_and_diffusion
+from ..data_loaders.humanml.scripts.motion_process import *
+from ..data_loaders.humanml.utils.utils import *
+from ..utils.model_util import load_model_blending_and_diffusion
 
 from diffusion import logger
 from utils import dist_util
-from data_loaders.get_data import get_dataset_loader
-from model.cfg_sampler import wrap_model
+from ..data_loaders.get_data import get_dataset_loader
+from ..model.cfg_sampler import wrap_model
 
 torch.multiprocessing.set_sharing_strategy('file_system')
 
diff --git a/eval/eval_humanact12_uestc.py b/eval/eval_humanact12_uestc.py
index d77370d..e78e5a6 100644
--- a/eval/eval_humanact12_uestc.py
+++ b/eval/eval_humanact12_uestc.py
@@ -7,12 +7,12 @@ import torch
 import re
 
 from utils import dist_util
-from model.cfg_sampler import ClassifierFreeSampleModel
-from data_loaders.get_data import get_dataset_loader
+from ..model.cfg_sampler import ClassifierFreeSampleModel
+from ..data_loaders.get_data import get_dataset_loader
 from eval.a2m.tools import save_metrics
-from utils.parser_util import evaluation_parser
-from utils.fixseed import fixseed
-from utils.model_util import create_model_and_diffusion, load_model_wo_clip
+from ..utils.parser_util import evaluation_parser
+from ..utils.fixseed import fixseed
+from ..utils.model_util import create_model_and_diffusion, load_model_wo_clip
 
 
 def evaluate(args, model, diffusion, data):
diff --git a/eval/eval_humanml_double_take.py b/eval/eval_humanml_double_take.py
index 55b1e4f..e32f402 100644
--- a/eval/eval_humanml_double_take.py
+++ b/eval/eval_humanml_double_take.py
@@ -1,20 +1,20 @@
-from model.DoubleTake_MDM import doubleTake_MDM
-from model.mdm import MDM
-from utils.parser_util import evaluation_double_take_parser
-from utils.fixseed import fixseed
+from ..model.DoubleTake_MDM import doubleTake_MDM
+from ..model.mdm import MDM
+from ..utils.parser_util import evaluation_double_take_parser
+from ..utils.fixseed import fixseed
 from datetime import datetime
-from data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
-from data_loaders.humanml.utils.metrics import *
-from data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
+from ..data_loaders.humanml.motion_loaders.model_motion_loaders import get_mdm_loader  # get_motion_loader
+from ..data_loaders.humanml.utils.metrics import *
+from ..data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
 from collections import OrderedDict
-from data_loaders.humanml.scripts.motion_process import *
-from data_loaders.humanml.utils.utils import *
-from utils.model_util import load_model
+from ..data_loaders.humanml.scripts.motion_process import *
+from ..data_loaders.humanml.utils.utils import *
+from ..utils.model_util import load_model
 
 from diffusion import logger
 from utils import dist_util
-from data_loaders.get_data import get_dataset_loader
-from model.cfg_sampler import ClassifierFreeSampleModel
+from ..data_loaders.get_data import get_dataset_loader
+from ..model.cfg_sampler import ClassifierFreeSampleModel
 
 torch.multiprocessing.set_sharing_strategy('file_system')
 
diff --git a/eval/eval_multi.py b/eval/eval_multi.py
index bd49ee1..eea077c 100644
--- a/eval/eval_multi.py
+++ b/eval/eval_multi.py
@@ -3,15 +3,15 @@ import os
 import torch
 import numpy as np
 
-from data_loaders.get_data import get_dataset_loader
-from data_loaders.humanml.scripts.motion_process import recover_from_ric
-from model.comMDM import ComMDM
+from ..data_loaders.get_data import get_dataset_loader
+from ..data_loaders.humanml.scripts.motion_process import recover_from_ric
+from ..model.comMDM import ComMDM
 import utils.rotation_conversions as geometry
-from model.cfg_sampler import UnconditionedModel
+from ..model.cfg_sampler import UnconditionedModel
 from utils import dist_util
-from utils.fixseed import fixseed
-from utils.model_util import load_model, load_model_wo_clip
-from utils.parser_util import evaluation_multi_parser
+from ..utils.fixseed import fixseed
+from ..utils.model_util import load_model, load_model_wo_clip
+from ..utils.parser_util import evaluation_multi_parser
 from diffusion import logger
 
 
diff --git a/infer.py b/infer.py
new file mode 100644
index 0000000..222a889
--- /dev/null
+++ b/infer.py
@@ -0,0 +1,274 @@
+# This code is based on https://github.com/openai/guided-diffusion
+"""
+Generate a large batch of image samples from a model and save them as a large
+numpy array. This can be used to produce samples for FID evaluation.
+"""
+from priorMDM.model.DoubleTake_MDM import doubleTake_MDM
+from priorMDM.utils.fixseed import fixseed
+import numpy as np
+import torch
+from priorMDM.utils.model_util import load_model
+from priorMDM.utils import dist_util
+from priorMDM.data_loaders.get_data import get_dataset_loader
+from priorMDM.data_loaders.humanml.scripts.motion_process import recover_from_ric
+from priorMDM.utils.sampling_utils import unfold_sample_arb_len, double_take_arb_len
+import logging
+from os.path import join, dirname
+
+import sys
+
+sys.modules["numpy.bool"] = bool
+sys.modules["numpy.int"] = int
+sys.modules["numpy.float"] = float
+sys.modules["numpy.complex"] = complex
+sys.modules["numpy.object"] = object
+sys.modules["numpy.str"] = str
+sys.modules["numpy.unicode"] = str
+
+
+class dotdict(dict):
+    """dot.notation access to dictionary attributes"""
+
+    __getattr__ = dict.get
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
+
+config = {
+    "cuda": True,
+    "device": 0,
+    "seed": 10,
+    "batch_size": 64,
+    "short_db": False,
+    "cropping_sampler": False,
+    "model_path": "./save/my_humanml_trans_enc_512/model000200000.pt",
+    "output_dir": "",
+    "num_samples": 10,
+    "num_repetitions": 1,
+    "guidance_param": 2.5,
+    "motion_length": 6.0,
+    "input_text": "./assets/dt_text_example.txt",
+    "action_file": "",
+    "text_prompt": "",
+    "action_name": "",
+    "sample_gt": False,
+    "min_seq_len": 45,
+    "max_seq_len": 250,
+    "double_take": True,
+    "second_take_only": False,
+    "handshake_size": 20,
+    "blend_len": 10,
+    "repaint_rep": 10,
+    "repaint": False,
+    "debug_double_take": False,
+    "skip_steps_double_take": 100,
+    "dataset": "humanml",
+    "data_dir": "",
+    "arch": "trans_enc",
+    "emb_trans_dec": False,
+    "layers": 8,
+    "latent_dim": 512,
+    "cond_mask_prob": 0.1,
+    "lambda_rcxyz": 0.0,
+    "lambda_vel": 0.0,
+    "lambda_fc": 0.0,
+    "use_tta": False,
+    "concat_trans_emb": False,
+    "trans_emb": False,
+    "noise_schedule": "cosine",
+    "diffusion_steps": 1000,
+    "sigma_small": True,
+}
+
+logger = logging.getLogger(__name__)
+
+
+def generate_motion(prompts: list[tuple[str, int]] = [], seed=None):
+    """
+    Generate motion from a text prompt using the PriorMDM technique
+    Based on sample/double_take.py from https://github.com/priorMDM/priorMDM/blob/main/sample/double_take.py
+
+    :param prompts: A list of tuples containing the text prompt and length of the motion to generate
+    :param seed: Seed to use for random number generation
+
+    :return: A dictionary containing the generated motion, text, lengths, number of samples and number of repetitions
+    """
+
+    logger.info("PriorMDM DoubleTake inference")
+    prompt_count = len(prompts)
+    if prompt_count == 1:
+        # We need to repeat the prompt otherwise the double take won't work
+        prompts = prompts * 2
+    args = dotdict(config)
+    if seed:
+        fixseed(seed)
+    n_frames = 150
+    dist_util.setup_dist(args.device)
+    args.num_samples = len(prompts)
+    args.batch_size = (
+        args.num_samples
+    )  # Sampling a single batch from the testset, with exactly args.num_samples
+    total_num_samples = args.num_samples * args.num_repetitions
+
+    logger.info("Creating model and diffusion")
+    model, diffusion = load_model(args, dist_util.dev(), ModelClass=doubleTake_MDM)
+
+    model_kwargs = {
+        "y": {
+            "mask": torch.ones(
+                (args.num_samples, 1, 1, 196)
+            ),  # 196 is humanml max frames number
+            "lengths": torch.tensor([prompt[1] for prompt in prompts]),
+            "text": [prompt[0] for prompt in prompts],
+            "tokens": [""],
+            "scale": torch.ones(args.num_samples) * 2.5,
+        }
+    }
+
+    all_motions = []
+    all_lengths = []
+    all_text = []
+    all_captions = []
+
+    for rep_i in range(args.num_repetitions):
+        logger.info("Sampling")
+        if args.guidance_param != 1:
+            model_kwargs["y"]["scale"] = (
+                torch.ones(args.batch_size, device=dist_util.dev())
+                * args.guidance_param
+            )
+        model_kwargs["y"] = {
+            key: val.to(dist_util.dev()) if torch.is_tensor(val) else val
+            for key, val in model_kwargs["y"].items()
+        }
+
+        max_arb_len = model_kwargs["y"]["lengths"].max()
+        samples_per_rep_list, samples_type = double_take_arb_len(
+            args, diffusion, model, model_kwargs, max_arb_len
+        )
+
+        step_sizes = np.zeros(len(model_kwargs["y"]["lengths"]), dtype=int)
+        for ii, len_i in enumerate(model_kwargs["y"]["lengths"]):
+            if ii == 0:
+                step_sizes[ii] = len_i
+                continue
+            step_sizes[ii] = step_sizes[ii - 1] + len_i - args.handshake_size
+
+        final_n_frames = step_sizes[-1]
+
+        for sample_i, samples_type_i in zip(samples_per_rep_list, samples_type):
+
+            sample = unfold_sample_arb_len(
+                sample_i, args.handshake_size, step_sizes, final_n_frames, model_kwargs
+            )
+
+            # Recover XYZ *positions* from HumanML3D vector representation
+            if model.data_rep == "hml_vec":
+                n_joints = 22 if sample.shape[1] == 263 else 21
+                sample = inv_transform(sample.cpu().permute(0, 2, 3, 1)).float()
+                sample = recover_from_ric(sample, n_joints)
+                sample = sample.view(-1, *sample.shape[2:]).permute(0, 2, 3, 1)
+            if args.dataset == "babel":
+                from data_loaders.amass.transforms import SlimSMPLTransform
+
+                transform = SlimSMPLTransform(
+                    batch_size=args.batch_size,
+                    name="SlimSMPLTransform",
+                    ename="smplnh",
+                    normalization=True,
+                )
+
+                all_feature = sample  # [bs, nfeats, 1, seq_len]
+                all_feature_squeeze = all_feature.squeeze(2)  # [bs, nfeats, seq_len]
+                all_feature_permutes = all_feature_squeeze.permute(
+                    0, 2, 1
+                )  # [bs, seq_len, nfeats]
+                splitted = torch.split(
+                    all_feature_permutes, all_feature.shape[0]
+                )  # [list of [seq_len,nfeats]]
+                sample_list = []
+                for seq in splitted[0]:
+                    all_features = seq
+                    Datastruct = transform.SlimDatastruct
+                    datastruct = Datastruct(features=all_features)
+                    sample = datastruct.joints
+
+                    sample_list.append(sample.permute(1, 2, 0).unsqueeze(0))
+                sample = torch.cat(sample_list)
+            else:
+                rot2xyz_pose_rep = (
+                    "xyz" if model.data_rep in ["xyz", "hml_vec"] else model.data_rep
+                )
+                if args.dataset == "babel":
+                    rot2xyz_pose_rep = "rot6d"
+                rot2xyz_mask = None
+
+                sample = model.rot2xyz(
+                    x=sample,
+                    mask=rot2xyz_mask,
+                    pose_rep=rot2xyz_pose_rep,
+                    glob=True,
+                    translation=True,
+                    jointstype="smpl",
+                    vertstrans=True,
+                    betas=None,
+                    beta=0,
+                    glob_rot=None,
+                    get_rotations_back=False,
+                )
+
+            text_key = "text" if "text" in model_kwargs["y"] else "action_text"
+
+            all_text += model_kwargs["y"][text_key]
+            all_captions += model_kwargs["y"][text_key]
+
+            all_motions.append(sample.cpu().numpy())
+            all_lengths.append(model_kwargs["y"]["lengths"].cpu().numpy())
+
+            logger.info(f"Created {len(all_motions) * args.batch_size} samples")
+
+    n_frames = final_n_frames
+    num_repetitions = args.num_repetitions
+
+    all_motions = np.concatenate(all_motions, axis=0)
+    all_motions = all_motions[:total_num_samples]  # [bs, njoints, 6, seqlen]
+    all_text = all_text[:total_num_samples]
+    all_lengths = [n_frames] * num_repetitions
+
+    # If we only have one prompt, we only want the first half of the data
+    if prompt_count == 1:
+        all_motions = all_motions[:, :, :, : step_sizes[0]]
+        all_text = all_text[:1]
+        step_sizes = step_sizes[:1]
+
+    return {
+        "motion": all_motions,
+        "text": all_text,
+        "lengths": step_sizes,
+        "num_samples": args.num_samples,
+        "num_repetitions": num_repetitions,
+    }
+
+
+def inv_transform(data):
+    mean = np.load(join(dirname(__file__), "dataset/HumanML3D", "Mean.npy"))
+    std = np.load(join(dirname(__file__), "dataset/HumanML3D", "Std.npy"))
+    return data * std + mean
+
+
+def load_dataset(args, n_frames):
+    if args.dataset == "babel":
+        args.num_frames = (args.min_seq_len, args.max_seq_len)
+    else:
+        args.num_frames = n_frames
+    data = get_dataset_loader(
+        name=args.dataset,
+        batch_size=args.batch_size,
+        num_frames=args.num_frames,
+        split="val",
+        load_mode="text_only",
+        short_db=args.short_db,
+        cropping_sampler=args.cropping_sampler,
+    )
+    data.fixed_length = n_frames
+    return data
diff --git a/model/DoubleTake_MDM.py b/model/DoubleTake_MDM.py
index 51cb1c3..473cabb 100644
--- a/model/DoubleTake_MDM.py
+++ b/model/DoubleTake_MDM.py
@@ -1,10 +1,10 @@
 import torch
 import torch.nn as nn
-from model.rotation2xyz import Rotation2xyz
+from .rotation2xyz import Rotation2xyz
 
-from model.mdm import MDM
-from model.mdm import InputProcess
-from model.mdm import PositionalEncoding
+from .mdm import MDM
+from .mdm import InputProcess
+from .mdm import PositionalEncoding
 
 class doubleTake_MDM(MDM):
     # def __init__(self, **kargs):
diff --git a/model/comMDM.py b/model/comMDM.py
index 20c7b19..0205b39 100644
--- a/model/comMDM.py
+++ b/model/comMDM.py
@@ -1,7 +1,7 @@
 import torch
 import torch.nn as nn
 
-from model.mdm import MDM
+from .mdm import MDM
 
 class ComMDM(MDM):
 
diff --git a/model/mdm.py b/model/mdm.py
index 14fd5bd..e6f61e2 100644
--- a/model/mdm.py
+++ b/model/mdm.py
@@ -1,11 +1,11 @@
 import numpy as np
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 import clip
-from model.rotation2xyz import Rotation2xyz
-
+from .rotation2xyz import Rotation2xyz
+import logging
 
+logger = logging.getLogger(__name__)
 
 class MDM(nn.Module):
     def __init__(self, modeltype, njoints, nfeats, num_actions, translation, pose_rep, glob, glob_rot,
@@ -53,7 +53,7 @@ class MDM(nn.Module):
         self.emb_trans_dec = emb_trans_dec
 
         if self.arch == 'trans_enc':
-            print("TRANS_ENC init")
+            logger.debug("TRANS_ENC init")
             seqTransEncoderLayer = nn.TransformerEncoderLayer(d_model=self.latent_dim,
                                                               nhead=self.num_heads,
                                                               dim_feedforward=self.ff_size,
@@ -63,7 +63,7 @@ class MDM(nn.Module):
             self.seqTransEncoder = nn.TransformerEncoder(seqTransEncoderLayer,
                                                          num_layers=self.num_layers)
         elif self.arch == 'trans_dec':
-            print("TRANS_DEC init")
+            logger.debug("TRANS_DEC init")
             seqTransDecoderLayer = nn.TransformerDecoderLayer(d_model=self.latent_dim,
                                                               nhead=self.num_heads,
                                                               dim_feedforward=self.ff_size,
@@ -72,7 +72,7 @@ class MDM(nn.Module):
             self.seqTransDecoder = nn.TransformerDecoder(seqTransDecoderLayer,
                                                          num_layers=self.num_layers)
         elif self.arch == 'gru':
-            print("GRU init")
+            logger.debug("GRU init")
             self.gru = nn.GRU(self.latent_dim, self.latent_dim, num_layers=self.num_layers, batch_first=True)
         else:
             raise ValueError('Please choose correct architecture [trans_enc, trans_dec, gru]')
@@ -82,13 +82,13 @@ class MDM(nn.Module):
         if self.cond_mode != 'no_cond':
             if 'text' in self.cond_mode:
                 self.embed_text = nn.Linear(self.clip_dim, self.latent_dim)
-                print('EMBED TEXT')
-                print('Loading CLIP...')
+                logger.debug("EMBED TEXT")
+                logger.info('Loading CLIP...')
                 self.clip_version = clip_version
                 self.clip_model = self.load_and_freeze_clip(clip_version)
             if 'action' in self.cond_mode:
                 self.embed_action = EmbedAction(self.num_actions, self.latent_dim)
-                print('EMBED ACTION')
+                logger.debug("EMBED ACTION")
 
         self.output_process = OutputProcess(self.data_rep, self.input_feats, self.latent_dim, self.njoints,
                                             self.nfeats)
@@ -187,12 +187,10 @@ class MDM(nn.Module):
         output = self.output_process(output)  # [bs, njoints, nfeats, nframes]
         return output
 
-
     def _apply(self, fn):
         super()._apply(fn)
         self.rot2xyz.smpl_model._apply(fn)
 
-
     def train(self, *args, **kwargs):
         super().train(*args, **kwargs)
         self.rot2xyz.smpl_model.train(*args, **kwargs)
@@ -299,4 +297,4 @@ class EmbedAction(nn.Module):
     def forward(self, input):
         idx = input[:, 0].to(torch.long)  # an index array must be long
         output = self.action_embedding[idx]
-        return output
\ No newline at end of file
+        return output
diff --git a/model/rotation2xyz.py b/model/rotation2xyz.py
index 844add9..3280912 100644
--- a/model/rotation2xyz.py
+++ b/model/rotation2xyz.py
@@ -1,32 +1,52 @@
 # This code is based on https://github.com/Mathux/ACTOR.git
 import torch
-import utils.rotation_conversions as geometry
-from data_loaders.amass.transforms.rots2joints import SMPLH
+import priorMDM.utils.rotation_conversions as geometry
+from priorMDM.data_loaders.amass.transforms.rots2joints import SMPLH
+
+from .smpl import SMPL, JOINTSTYPE_ROOT
 
-from model.smpl import SMPL, JOINTSTYPE_ROOT
 JOINTSTYPES = ["a2m", "a2mpl", "smpl", "vibe", "vertices"]
 
 
 class Rotation2xyz:
-    def __init__(self, device, dataset='amass', batch_size=None):
+    def __init__(self, device, dataset="amass", batch_size=None):
         self.device = device
         self.dataset = dataset
-        if dataset == 'babel':
-            self.smpl_model = SMPLH(path='./body_models/smpl_models/smplh',
-                                                  jointstype='smplnh',
-                                                  input_pose_rep='matrix',
-                                                  batch_size=batch_size,
-                                                  gender='male',
-                                                  name='SMPLH').eval().to(device)
+        if dataset == "babel":
+            self.smpl_model = (
+                SMPLH(
+                    path="./body_models/smpl_models/smplh",
+                    jointstype="smplnh",
+                    input_pose_rep="matrix",
+                    batch_size=batch_size,
+                    gender="male",
+                    name="SMPLH",
+                )
+                .eval()
+                .to(device)
+            )
 
         else:
             self.smpl_model = SMPL().eval().to(device)
 
-    def __call__(self, x, mask, pose_rep, translation, glob,
-                 jointstype, vertstrans, betas=None, beta=0,
-                 glob_rot=None, get_rotations_back=False, data_type=None, **kwargs):
-
-        if self.dataset == 'babel':
+    def __call__(
+        self,
+        x,
+        mask,
+        pose_rep,
+        translation,
+        glob,
+        jointstype,
+        vertstrans,
+        betas=None,
+        beta=0,
+        glob_rot=None,
+        get_rotations_back=False,
+        data_type=None,
+        **kwargs
+    ):
+
+        if self.dataset == "babel":
             out = self.smpl_model(smpl_data=x, batch_size=1)
             return out
         if pose_rep == "xyz":
@@ -64,26 +84,40 @@ class Rotation2xyz:
 
         if not glob:
             global_orient = torch.tensor(glob_rot, device=x.device)
-            global_orient = geometry.axis_angle_to_matrix(global_orient).view(1, 1, 3, 3)
+            global_orient = geometry.axis_angle_to_matrix(global_orient).view(
+                1, 1, 3, 3
+            )
             global_orient = global_orient.repeat(len(rotations), 1, 1, 1)
         else:
             global_orient = rotations[:, 0]
             rotations = rotations[:, 1:]
 
         if betas is None:
-            betas = torch.zeros([rotations.shape[0], self.smpl_model.num_betas],
-                                dtype=rotations.dtype, device=rotations.device)
+            betas = torch.zeros(
+                [rotations.shape[0], self.smpl_model.num_betas],
+                dtype=rotations.dtype,
+                device=rotations.device,
+            )
             betas[:, 1] = beta
             # import ipdb; ipdb.set_trace()
-        if self.dataset == 'babel':
-            out = self.smpl_model(body_pose=rotations, global_orient=global_orient, betas=betas, input_pose_rep='rot6d')
+        if self.dataset == "babel":
+            out = self.smpl_model(
+                body_pose=rotations,
+                global_orient=global_orient,
+                betas=betas,
+                input_pose_rep="rot6d",
+            )
         else:
-            out = self.smpl_model(body_pose=rotations, global_orient=global_orient, betas=betas)
+            out = self.smpl_model(
+                body_pose=rotations, global_orient=global_orient, betas=betas
+            )
 
         # get the desirable joints
         joints = out[jointstype]
 
-        x_xyz = torch.empty(nsamples, time, joints.shape[1], 3, device=x.device, dtype=x.dtype)
+        x_xyz = torch.empty(
+            nsamples, time, joints.shape[1], 3, device=x.device, dtype=x.dtype
+        )
         x_xyz[~mask] = 0
         x_xyz[mask] = joints
 
diff --git a/model/smpl.py b/model/smpl.py
index 587f541..2ca7dbb 100644
--- a/model/smpl.py
+++ b/model/smpl.py
@@ -12,7 +12,7 @@ from smplx.lbs import vertices2joints
 # change 0 and 8
 action2motion_joints = [8, 1, 2, 3, 4, 5, 6, 7, 0, 9, 10, 11, 12, 13, 14, 21, 24, 38]
 
-from utils.config import SMPL_MODEL_PATH, JOINT_REGRESSOR_TRAIN_EXTRA
+from ..utils.config import SMPL_MODEL_PATH, JOINT_REGRESSOR_TRAIN_EXTRA
 
 JOINTSTYPE_ROOT = {"a2m": 0, # action2motion
                    "smpl": 0,
diff --git a/sample/double_take.py b/sample/double_take.py
index 6fea399..c640e26 100644
--- a/sample/double_take.py
+++ b/sample/double_take.py
@@ -3,65 +3,86 @@
 Generate a large batch of image samples from a model and save them as a large
 numpy array. This can be used to produce samples for FID evaluation.
 """
-from model.DoubleTake_MDM import doubleTake_MDM
-from utils.fixseed import fixseed
+from ..model.DoubleTake_MDM import doubleTake_MDM
+from ..utils.fixseed import fixseed
 import os
 import numpy as np
 import torch
-from utils.parser_util import generate_args
-from utils.model_util import load_model
+from ..utils.parser_util import generate_args
+from ..utils.model_util import load_model
 from utils import dist_util
-from model.cfg_sampler import ClassifierFreeSampleModel
-from data_loaders.get_data import get_dataset_loader
-from data_loaders.humanml.scripts.motion_process import recover_from_ric
+from ..data_loaders.get_data import get_dataset_loader
+from ..data_loaders.humanml.scripts.motion_process import recover_from_ric
 import data_loaders.humanml.utils.paramUtil as paramUtil
-from data_loaders.humanml.utils.plot_script import plot_3d_motion
 import shutil
-from utils.sampling_utils import unfold_sample_arb_len, double_take_arb_len
+from ..utils.sampling_utils import unfold_sample_arb_len, double_take_arb_len
 import pandas as pd
 
+import sys
+
+sys.modules["numpy.bool"] = bool
+sys.modules["numpy.int"] = int
+sys.modules["numpy.float"] = float
+sys.modules["numpy.complex"] = complex
+sys.modules["numpy.object"] = object
+sys.modules["numpy.str"] = str
+sys.modules["numpy.unicode"] = str
+
 
 def calc_frame_colors(handshake_size, blend_size, step_sizes, lengths):
     for ii, step_size in enumerate(step_sizes):
         if ii == 0:
-            frame_colors = ['orange'] * (step_size - handshake_size - blend_size) + \
-                           ['blue'] * blend_size + \
-                           ['purple'] * (handshake_size // 2)
+            frame_colors = (
+                ["orange"] * (step_size - handshake_size - blend_size)
+                + ["blue"] * blend_size
+                + ["purple"] * (handshake_size // 2)
+            )
             continue
         if ii == len(step_sizes) - 1:
-            frame_colors += ['purple'] * (handshake_size // 2) + \
-                            ['blue'] * blend_size + \
-                            ['orange'] * (lengths[ii] - handshake_size - blend_size)
+            frame_colors += (
+                ["purple"] * (handshake_size // 2)
+                + ["blue"] * blend_size
+                + ["orange"] * (lengths[ii] - handshake_size - blend_size)
+            )
             continue
-        frame_colors += ['purple'] * (handshake_size // 2) + ['blue'] * blend_size + \
-                        ['orange'] * (lengths[ii] - 2 * handshake_size - 2 * blend_size) + \
-                        ['blue'] * blend_size + \
-                        ['purple'] * (handshake_size // 2)
+        frame_colors += (
+            ["purple"] * (handshake_size // 2)
+            + ["blue"] * blend_size
+            + ["orange"] * (lengths[ii] - 2 * handshake_size - 2 * blend_size)
+            + ["blue"] * blend_size
+            + ["purple"] * (handshake_size // 2)
+        )
     return frame_colors
 
 
 def main():
-    print(f"generating samples")
+    print("generating samples")
     args = generate_args()
     fixseed(args.seed)
     out_path = args.output_dir
     name = os.path.basename(os.path.dirname(args.model_path))
-    niter = os.path.basename(args.model_path).replace('model', '').replace('.pt', '')
-    fps = 30 if args.dataset == 'babel' else 20
+    niter = os.path.basename(args.model_path).replace("model", "").replace(".pt", "")
+    fps = 30 if args.dataset == "babel" else 20
     n_frames = 150
     is_using_data = not args.input_text
     dist_util.setup_dist(args.device)
     is_csv, is_txt = False, False
-    assert (args.double_take)
-    if out_path == '':
-        out_path = os.path.join(os.path.dirname(args.model_path),
-                                'DoubleTake_samples_{}_{}_seed{}'.format(name, niter, args.seed))
-        if args.input_text != '':
+    assert args.double_take
+    if out_path == "":
+        out_path = os.path.join(
+            os.path.dirname(args.model_path),
+            "DoubleTake_samples_{}_{}_seed{}".format(name, niter, args.seed),
+        )
+        if args.input_text != "":
             if ".txt" in args.input_text:
-                out_path += '_' + os.path.basename(args.input_text).replace('.txt', '').replace(' ', '_').replace('.', '')
+                out_path += "_" + os.path.basename(args.input_text).replace(
+                    ".txt", ""
+                ).replace(" ", "_").replace(".", "")
                 is_txt = True
             elif ".csv" in args.input_text:
-                out_path += '_' + os.path.basename(args.input_text).replace('.csv', '').replace(' ', '_').replace('.', '')
+                out_path += "_" + os.path.basename(args.input_text).replace(
+                    ".csv", ""
+                ).replace(" ", "_").replace(".", "")
                 is_csv = True
             else:
                 raise TypeError("Incorrect text file type, use csv or txt")
@@ -74,45 +95,57 @@ def main():
             out_path += f"_skipSteps_{args.skip_steps_double_take}"
 
     # this block must be called BEFORE the dataset is loaded
-    if args.input_text != '':
+    if args.input_text != "":
         assert os.path.exists(args.input_text)
         if is_txt:
-            with open(args.input_text, 'r') as fr:
+            with open(args.input_text, "r") as fr:
                 texts = fr.readlines()
-            texts = [s.replace('\n', '') for s in texts]
+            texts = [s.replace("\n", "") for s in texts]
             args.num_samples = len(texts)
         elif is_csv:
             df = pd.read_csv(args.input_text)
-            args.num_samples = len(list(df['text']))
+            args.num_samples = len(list(df["text"]))
 
-    args.batch_size = args.num_samples  # Sampling a single batch from the testset, with exactly args.num_samples
+    args.batch_size = (
+        args.num_samples
+    )  # Sampling a single batch from the testset, with exactly args.num_samples
 
-    print('Loading dataset...')
+    print("Loading dataset...")
     data = load_dataset(args, n_frames)
     total_num_samples = args.num_samples * args.num_repetitions
 
     print("Creating model and diffusion...")
-    model, diffusion = load_model(args, data, dist_util.dev(), ModelClass=doubleTake_MDM)
+    model, diffusion = load_model(
+        args, data, dist_util.dev(), ModelClass=doubleTake_MDM
+    )
 
     if is_using_data:
         iterator = iter(data)
         gt_motion, model_kwargs = next(iterator)
     elif is_csv:
-        model_kwargs = {'y': {
-            'mask': torch.ones((len(list(df['text'])), 1, 1, 196)), #196 is humanml max frames number
-            'lengths': torch.tensor(list(df['length'])),
-            'text': list(df['text']),
-            'tokens': [''],
-            'scale': torch.ones(len(list(df['text'])))*2.5
-        }}
+        model_kwargs = {
+            "y": {
+                "mask": torch.ones(
+                    (len(list(df["text"])), 1, 1, 196)
+                ),  # 196 is humanml max frames number
+                "lengths": torch.tensor(list(df["length"])),
+                "text": list(df["text"]),
+                "tokens": [""],
+                "scale": torch.ones(len(list(df["text"]))) * 2.5,
+            }
+        }
     elif is_txt:
-        model_kwargs = {'y': {
-            'mask': torch.ones((len(texts), 1, 1, 196)), # 196 is humanml max frames number
-            'lengths': torch.tensor([n_frames]*len(texts)),
-            'text': texts,
-            'tokens': [''],
-            'scale': torch.ones(len(texts))*2.5
-        }}
+        model_kwargs = {
+            "y": {
+                "mask": torch.ones(
+                    (len(texts), 1, 1, 196)
+                ),  # 196 is humanml max frames number
+                "lengths": torch.tensor([n_frames] * len(texts)),
+                "text": texts,
+                "tokens": [""],
+                "scale": torch.ones(len(texts)) * 2.5,
+            }
+        }
     else:
         raise TypeError("Only text to motion is availible atm")
 
@@ -122,48 +155,70 @@ def main():
     all_captions = []
 
     for rep_i in range(args.num_repetitions):
-        print(f'### Sampling [repetitions #{rep_i}]')
+        print(f"### Sampling [repetitions #{rep_i}]")
         if args.guidance_param != 1:
-            model_kwargs['y']['scale'] = torch.ones(args.batch_size, device=dist_util.dev()) * args.guidance_param
-        model_kwargs['y'] = {key: val.to(dist_util.dev()) if torch.is_tensor(val) else val for key, val in model_kwargs['y'].items()}
-
-        max_arb_len = model_kwargs['y']['lengths'].max()
-        min_arb_len = 2 * args.handshake_size + 2*args.blend_len + 10
-
-        for ii, len_s in enumerate(model_kwargs['y']['lengths']):
+            model_kwargs["y"]["scale"] = (
+                torch.ones(args.batch_size, device=dist_util.dev())
+                * args.guidance_param
+            )
+        model_kwargs["y"] = {
+            key: val.to(dist_util.dev()) if torch.is_tensor(val) else val
+            for key, val in model_kwargs["y"].items()
+        }
+
+        max_arb_len = model_kwargs["y"]["lengths"].max()
+        min_arb_len = 2 * args.handshake_size + 2 * args.blend_len + 10
+
+        for ii, len_s in enumerate(model_kwargs["y"]["lengths"]):
             if len_s > max_arb_len:
-                model_kwargs['y']['lengths'][ii] = max_arb_len
+                model_kwargs["y"]["lengths"][ii] = max_arb_len
             if len_s < min_arb_len:
-                model_kwargs['y']['lengths'][ii] = min_arb_len
-        samples_per_rep_list, samples_type = double_take_arb_len(args, diffusion, model, model_kwargs, max_arb_len)
+                model_kwargs["y"]["lengths"][ii] = min_arb_len
+        samples_per_rep_list, samples_type = double_take_arb_len(
+            args, diffusion, model, model_kwargs, max_arb_len
+        )
 
-        step_sizes = np.zeros(len(model_kwargs['y']['lengths']), dtype=int)
-        for ii, len_i in enumerate(model_kwargs['y']['lengths']):
+        step_sizes = np.zeros(len(model_kwargs["y"]["lengths"]), dtype=int)
+        for ii, len_i in enumerate(model_kwargs["y"]["lengths"]):
             if ii == 0:
                 step_sizes[ii] = len_i
                 continue
-            step_sizes[ii] = step_sizes[ii-1] + len_i - args.handshake_size
+            step_sizes[ii] = step_sizes[ii - 1] + len_i - args.handshake_size
 
         final_n_frames = step_sizes[-1]
 
         for sample_i, samples_type_i in zip(samples_per_rep_list, samples_type):
 
-            sample = unfold_sample_arb_len(sample_i, args.handshake_size, step_sizes, final_n_frames, model_kwargs)
+            sample = unfold_sample_arb_len(
+                sample_i, args.handshake_size, step_sizes, final_n_frames, model_kwargs
+            )
 
             # Recover XYZ *positions* from HumanML3D vector representation
-            if model.data_rep == 'hml_vec':
+            if model.data_rep == "hml_vec":
                 n_joints = 22 if sample.shape[1] == 263 else 21
-                sample = data.dataset.t2m_dataset.inv_transform(sample.cpu().permute(0, 2, 3, 1)).float()
+                sample = data.dataset.t2m_dataset.inv_transform(
+                    sample.cpu().permute(0, 2, 3, 1)
+                ).float()
                 sample = recover_from_ric(sample, n_joints)
                 sample = sample.view(-1, *sample.shape[2:]).permute(0, 2, 3, 1)
-            if args.dataset == 'babel':
+            if args.dataset == "babel":
                 from data_loaders.amass.transforms import SlimSMPLTransform
-                transform = SlimSMPLTransform(batch_size=args.batch_size, name='SlimSMPLTransform', ename='smplnh', normalization=True)
 
-                all_feature = sample #[bs, nfeats, 1, seq_len]
-                all_feature_squeeze = all_feature.squeeze(2) #[bs, nfeats, seq_len]
-                all_feature_permutes = all_feature_squeeze.permute(0, 2, 1) #[bs, seq_len, nfeats]
-                splitted = torch.split(all_feature_permutes, all_feature.shape[0]) #[list of [seq_len,nfeats]]
+                transform = SlimSMPLTransform(
+                    batch_size=args.batch_size,
+                    name="SlimSMPLTransform",
+                    ename="smplnh",
+                    normalization=True,
+                )
+
+                all_feature = sample  # [bs, nfeats, 1, seq_len]
+                all_feature_squeeze = all_feature.squeeze(2)  # [bs, nfeats, seq_len]
+                all_feature_permutes = all_feature_squeeze.permute(
+                    0, 2, 1
+                )  # [bs, seq_len, nfeats]
+                splitted = torch.split(
+                    all_feature_permutes, all_feature.shape[0]
+                )  # [list of [seq_len,nfeats]]
                 sample_list = []
                 for seq in splitted[0]:
                     all_features = seq
@@ -174,22 +229,34 @@ def main():
                     sample_list.append(sample.permute(1, 2, 0).unsqueeze(0))
                 sample = torch.cat(sample_list)
             else:
-                rot2xyz_pose_rep = 'xyz' if model.data_rep in ['xyz', 'hml_vec'] else model.data_rep
-                if args.dataset == 'babel':
-                    rot2xyz_pose_rep = 'rot6d'
+                rot2xyz_pose_rep = (
+                    "xyz" if model.data_rep in ["xyz", "hml_vec"] else model.data_rep
+                )
+                if args.dataset == "babel":
+                    rot2xyz_pose_rep = "rot6d"
                 rot2xyz_mask = None
 
-                sample = model.rot2xyz(x=sample, mask=rot2xyz_mask, pose_rep=rot2xyz_pose_rep, glob=True, translation=True,
-                                       jointstype='smpl', vertstrans=True, betas=None, beta=0, glob_rot=None,
-                                       get_rotations_back=False)
-
-            text_key = 'text' if 'text' in model_kwargs['y'] else 'action_text'
-
-            all_text += model_kwargs['y'][text_key]
-            all_captions += model_kwargs['y'][text_key]
+                sample = model.rot2xyz(
+                    x=sample,
+                    mask=rot2xyz_mask,
+                    pose_rep=rot2xyz_pose_rep,
+                    glob=True,
+                    translation=True,
+                    jointstype="smpl",
+                    vertstrans=True,
+                    betas=None,
+                    beta=0,
+                    glob_rot=None,
+                    get_rotations_back=False,
+                )
+
+            text_key = "text" if "text" in model_kwargs["y"] else "action_text"
+
+            all_text += model_kwargs["y"][text_key]
+            all_captions += model_kwargs["y"][text_key]
 
             all_motions.append(sample.cpu().numpy())
-            all_lengths.append(model_kwargs['y']['lengths'].cpu().numpy())
+            all_lengths.append(model_kwargs["y"]["lengths"].cpu().numpy())
 
             print(f"created {len(all_motions) * args.batch_size} samples")
 
@@ -211,66 +278,113 @@ def main():
         shutil.rmtree(out_path)
     os.makedirs(out_path)
 
-    npy_path = os.path.join(out_path, 'results.npy')
-    frame_colors = calc_frame_colors(args.handshake_size, args.blend_len, step_sizes, model_kwargs['y']['lengths'])
+    npy_path = os.path.join(out_path, "results.npy")
+    frame_colors = calc_frame_colors(
+        args.handshake_size, args.blend_len, step_sizes, model_kwargs["y"]["lengths"]
+    )
     print(f"saving results file to [{npy_path}]")
-    np.save(npy_path,
-            {'motion': all_motions, 'text': all_text, 'lengths': all_lengths,
-             'num_samples': args.num_samples, 'num_repetitions': num_repetitions, 'frame_colors': frame_colors})
-    with open(npy_path.replace('.npy', '.txt'), 'w') as fw:
-        fw.write('\n'.join(all_text))
-    with open(npy_path.replace('.npy', '_len.txt'), 'w') as fw:
-        fw.write('\n'.join([str(l) for l in all_lengths]))
+    np.save(
+        npy_path,
+        {
+            "motion": all_motions,
+            "text": all_text,
+            "lengths": all_lengths,
+            "num_samples": args.num_samples,
+            "num_repetitions": num_repetitions,
+            "frame_colors": frame_colors,
+        },
+    )
+    with open(npy_path.replace(".npy", ".txt"), "w") as fw:
+        fw.write("\n".join(all_text))
+    with open(npy_path.replace(".npy", "_len.txt"), "w") as fw:
+        fw.write("\n".join([str(l) for l in all_lengths]))
 
     print(f"saving visualizations to [{out_path}]...")
-    skeleton = paramUtil.kit_kinematic_chain if args.dataset == 'kit' else paramUtil.t2m_kinematic_chain
-    if args.dataset == 'babel':
+    skeleton = (
+        paramUtil.kit_kinematic_chain
+        if args.dataset == "kit"
+        else paramUtil.t2m_kinematic_chain
+    )
+    if args.dataset == "babel":
         skeleton = paramUtil.t2m_kinematic_chain
     sample_files = []
-    for sample_i in range(args.num_samples):
-        rep_files = []
-        for rep_i, samples_type_i in zip(range(num_repetitions), samples_type):
-            caption = [f'{samples_type_i} {all_text[0]}'] * (model_kwargs['y']['lengths'][0] - int(args.handshake_size/2))
-            for ii in range(1, old_num_samples):
-                caption += [f'{samples_type_i} {all_text[ii]}'] * (int(model_kwargs['y']['lengths'][ii])-args.handshake_size)
-            caption += [f'{samples_type_i} {all_text[ii]}'] * (int(args.handshake_size/2))
-            length = all_lengths[rep_i*args.batch_size + sample_i]
-            motion = all_motions[rep_i*args.batch_size + sample_i].transpose(2, 0, 1)[:length]
-            save_file = 'sample{:02d}_rep{:02d}.mp4'.format(sample_i, rep_i)
-            animation_save_path = os.path.join(out_path, save_file)
-            print(f'[({sample_i}) "{set(caption)}" | Rep #{rep_i} | -> {save_file}]')
-            plot_3d_motion(animation_save_path, skeleton, motion, dataset=args.dataset, title=caption, fps=fps,
-                           vis_mode='gt' if args.sample_gt else 'unfold_arb_len', handshake_size=args.handshake_size,
-                           blend_size=args.blend_len,step_sizes=step_sizes, lengths=model_kwargs['y']['lengths'])
-            # Credit for visualization: https://github.com/EricGuo5513/text-to-motion
-            rep_files.append(animation_save_path)
-        if num_repetitions > 1:
-            all_rep_save_file = os.path.join(out_path, 'sample{:02d}.mp4'.format(sample_i))
-            ffmpeg_rep_files = [f' -i {f} ' for f in rep_files]
-            hstack_args = f' -filter_complex hstack=inputs={num_repetitions}' if num_repetitions > 1 else ''
-            ffmpeg_rep_cmd = f'ffmpeg -y -loglevel warning ' + ''.join(ffmpeg_rep_files) + f'{hstack_args} {all_rep_save_file}'
-            os.system(ffmpeg_rep_cmd)
-            print(f'[({sample_i}) "{set(caption)}" | all repetitions | -> {all_rep_save_file}]')
-            sample_files.append(all_rep_save_file)
-
+    # for sample_i in range(args.num_samples):
+    #     rep_files = []
+    #     for rep_i, samples_type_i in zip(range(num_repetitions), samples_type):
+    #         caption = [f"{samples_type_i} {all_text[0]}"] * (
+    #             model_kwargs["y"]["lengths"][0] - int(args.handshake_size / 2)
+    #         )
+    #         for ii in range(1, old_num_samples):
+    #             caption += [f"{samples_type_i} {all_text[ii]}"] * (
+    #                 int(model_kwargs["y"]["lengths"][ii]) - args.handshake_size
+    #             )
+    #         caption += [f"{samples_type_i} {all_text[ii]}"] * (
+    #             int(args.handshake_size / 2)
+    #         )
+    #         length = all_lengths[rep_i * args.batch_size + sample_i]
+    #         motion = all_motions[rep_i * args.batch_size + sample_i].transpose(2, 0, 1)[
+    #             :length
+    #         ]
+    #         save_file = "sample{:02d}_rep{:02d}.mp4".format(sample_i, rep_i)
+    #         animation_save_path = os.path.join(out_path, save_file)
+    #         print(f'[({sample_i}) "{set(caption)}" | Rep #{rep_i} | -> {save_file}]')
+    #         plot_3d_motion(
+    #             animation_save_path,
+    #             skeleton,
+    #             motion,
+    #             dataset=args.dataset,
+    #             title=caption,
+    #             fps=fps,
+    #             vis_mode="gt" if args.sample_gt else "unfold_arb_len",
+    #             handshake_size=args.handshake_size,
+    #             blend_size=args.blend_len,
+    #             step_sizes=step_sizes,
+    #             lengths=model_kwargs["y"]["lengths"],
+    #         )
+    #         # Credit for visualization: https://github.com/EricGuo5513/text-to-motion
+    #         rep_files.append(animation_save_path)
+    #     if num_repetitions > 1:
+    #         all_rep_save_file = os.path.join(
+    #             out_path, "sample{:02d}.mp4".format(sample_i)
+    #         )
+    #         ffmpeg_rep_files = [f" -i {f} " for f in rep_files]
+    #         hstack_args = (
+    #             f" -filter_complex hstack=inputs={num_repetitions}"
+    #             if num_repetitions > 1
+    #             else ""
+    #         )
+    #         ffmpeg_rep_cmd = (
+    #             "ffmpeg -y -loglevel warning "
+    #             + "".join(ffmpeg_rep_files)
+    #             + f"{hstack_args} {all_rep_save_file}"
+    #         )
+    #         os.system(ffmpeg_rep_cmd)
+    #         print(
+    #             f'[({sample_i}) "{set(caption)}" | all repetitions | -> {all_rep_save_file}]'
+    #         )
+    #         sample_files.append(all_rep_save_file)
 
     abs_path = os.path.abspath(out_path)
-    print(f'[Done] Results are at [{abs_path}]')
+    print(f"[Done] Results are at [{abs_path}]")
+
 
 def load_dataset(args, n_frames):
-    if args.dataset == 'babel':
+    if args.dataset == "babel":
         args.num_frames = (args.min_seq_len, args.max_seq_len)
     else:
         args.num_frames = n_frames
-    data = get_dataset_loader(name=args.dataset,
-                              batch_size=args.batch_size,
-                              num_frames=args.num_frames,
-                              split='val',
-                              load_mode='text_only',
-                              short_db=args.short_db,
-                              cropping_sampler=args.cropping_sampler)
+    data = get_dataset_loader(
+        name=args.dataset,
+        batch_size=args.batch_size,
+        num_frames=args.num_frames,
+        split="val",
+        load_mode="text_only",
+        short_db=args.short_db,
+        cropping_sampler=args.cropping_sampler,
+    )
     data.fixed_length = n_frames
     return data
 
+
 if __name__ == "__main__":
     main()
diff --git a/sample/finetuned_motion_control.py b/sample/finetuned_motion_control.py
index f49e6fb..f0916e3 100644
--- a/sample/finetuned_motion_control.py
+++ b/sample/finetuned_motion_control.py
@@ -3,22 +3,22 @@
 Generate a large batch of image samples from a model and save them as a large
 numpy array. This can be used to produce samples for FID evaluation.
 """
-from diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
-from diffusion.respace import SpacedDiffusion
-from model.model_blending import ModelBlender
-from utils.fixseed import fixseed
+from ..diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
+from ..diffusion.respace import SpacedDiffusion
+from ..model.model_blending import ModelBlender
+from ..utils.fixseed import fixseed
 import os
 import numpy as np
 import torch
-from utils.parser_util import edit_inpainting_args
-from utils.model_util import load_model_blending_and_diffusion
+from ..utils.parser_util import edit_inpainting_args
+from ..utils.model_util import load_model_blending_and_diffusion
 from utils import dist_util
-from model.cfg_sampler import wrap_model
-from data_loaders.get_data import get_dataset_loader
-from data_loaders.humanml.scripts.motion_process import recover_from_ric
-from data_loaders.humanml_utils import get_inpainting_mask
+from ..model.cfg_sampler import wrap_model
+from ..data_loaders.get_data import get_dataset_loader
+from ..data_loaders.humanml.scripts.motion_process import recover_from_ric
+from ..data_loaders.humanml_utils import get_inpainting_mask
 import data_loaders.humanml.utils.paramUtil as paramUtil
-from data_loaders.humanml.utils.plot_script import plot_3d_motion
+from ..data_loaders.humanml.utils.plot_script import plot_3d_motion
 import shutil
 
 def main():
diff --git a/sample/two_person_prefix_completion.py b/sample/two_person_prefix_completion.py
index c5d185d..c9dddbb 100644
--- a/sample/two_person_prefix_completion.py
+++ b/sample/two_person_prefix_completion.py
@@ -4,19 +4,19 @@ Generate a large batch of image samples from a model and save them as a large
 numpy array. This can be used to produce samples for FID evaluation.
 """
 from data_loaders import humanml_utils
-from eval.eval_multi import extract_motions
-from model.comMDM import ComMDM
-from utils.fixseed import fixseed
+from ..eval.eval_multi import extract_motions
+from ..model.comMDM import ComMDM
+from ..utils.fixseed import fixseed
 import os
 import numpy as np
 import torch
-from utils.parser_util import edit_multi_args
-from utils.model_util import load_model
+from ..utils.parser_util import edit_multi_args
+from ..utils.model_util import load_model
 from utils import dist_util
-from model.cfg_sampler import UnconditionedModel
-from data_loaders.get_data import get_dataset_loader
+from ..model.cfg_sampler import UnconditionedModel
+from ..data_loaders.get_data import get_dataset_loader
 import data_loaders.humanml.utils.paramUtil as paramUtil
-from data_loaders.humanml.utils.plot_script import plot_3d_motion
+from ..data_loaders.humanml.utils.plot_script import plot_3d_motion
 import shutil
 
 
diff --git a/sample/two_person_text2motion.py b/sample/two_person_text2motion.py
index ea7303d..62a3bdb 100644
--- a/sample/two_person_text2motion.py
+++ b/sample/two_person_text2motion.py
@@ -3,21 +3,21 @@
 Generate a large batch of image samples from a model and save them as a large
 numpy array. This can be used to produce samples for FID evaluation.
 """
-from model.comMDM import ComMDM
-from utils.fixseed import fixseed
+from ..model.comMDM import ComMDM
+from ..utils.fixseed import fixseed
 import os
 import numpy as np
 import torch
-from utils.parser_util import generate_multi_args
-from utils.model_util import create_model_and_diffusion, load_model
+from ..utils.parser_util import generate_multi_args
+from ..utils.model_util import create_model_and_diffusion, load_model
 from utils import dist_util
-from model.cfg_sampler import ClassifierFreeSampleModel
-from data_loaders.get_data import get_dataset_loader
-from data_loaders.humanml.scripts.motion_process import recover_from_ric
+from ..model.cfg_sampler import ClassifierFreeSampleModel
+from ..data_loaders.get_data import get_dataset_loader
+from ..data_loaders.humanml.scripts.motion_process import recover_from_ric
 import data_loaders.humanml.utils.paramUtil as paramUtil
-from data_loaders.humanml.utils.plot_script import plot_3d_motion
+from ..data_loaders.humanml.utils.plot_script import plot_3d_motion
 import shutil
-from data_loaders.tensors import collate
+from ..data_loaders.tensors import collate
 import utils.rotation_conversions as geometry
 
 
diff --git a/train/train_mdm.py b/train/train_mdm.py
index d356f85..8edb631 100644
--- a/train/train_mdm.py
+++ b/train/train_mdm.py
@@ -5,13 +5,13 @@ Train a diffusion model on images.
 
 import os
 import json
-from utils.fixseed import fixseed
-from utils.parser_util import train_args
+from ..utils.fixseed import fixseed
+from ..utils.parser_util import train_args
 from utils import dist_util
-from train.training_loop import TrainLoop
-from data_loaders.get_data import get_dataset_loader
-from utils.model_util import create_model_and_diffusion
-from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
+from .training_loop import TrainLoop
+from ..data_loaders.get_data import get_dataset_loader
+from ..utils.model_util import create_model_and_diffusion
+from .train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
 
 def main():
     args = train_args()
diff --git a/train/train_mdm_motion_control.py b/train/train_mdm_motion_control.py
index 6fcfe6a..b2d8281 100644
--- a/train/train_mdm_motion_control.py
+++ b/train/train_mdm_motion_control.py
@@ -4,18 +4,18 @@ Train a diffusion model on images.
 """
 import os
 import json
-from diffusion.respace import SpacedDiffusion
-from utils.fixseed import fixseed
-from utils.parser_util import train_inpainting_args
+from ..diffusion.respace import SpacedDiffusion
+from ..utils.fixseed import fixseed
+from ..utils.parser_util import train_inpainting_args
 from utils import dist_util
-from train.training_loop import TrainLoop
-from data_loaders.get_data import get_dataset_loader
-from utils.model_util import create_model_and_diffusion
-from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
+from .training_loop import TrainLoop
+from ..data_loaders.get_data import get_dataset_loader
+from ..utils.model_util import create_model_and_diffusion
+from .train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
 import torch
-from data_loaders.humanml_utils import get_inpainting_mask
+from ..data_loaders.humanml_utils import get_inpainting_mask
 from torch.utils.data import DataLoader
-from diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
+from ..diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
 
 def main():
     args = train_inpainting_args()
diff --git a/train/train_mdm_multi.py b/train/train_mdm_multi.py
index fe50170..55d3a9a 100644
--- a/train/train_mdm_multi.py
+++ b/train/train_mdm_multi.py
@@ -5,14 +5,14 @@ Train a diffusion model on images.
 
 import os
 import json
-from model.comMDM import ComMDM
-from utils.fixseed import fixseed
-from utils.parser_util import train_multi_args
+from ..model.comMDM import ComMDM
+from ..utils.fixseed import fixseed
+from ..utils.parser_util import train_multi_args
 from utils import dist_util
-from train.training_loop import TrainLoop
-from data_loaders.get_data import get_dataset_loader
-from utils.model_util import create_model_and_diffusion, load_model_wo_clip, load_pretrained_mdm, load_split_mdm
-from train.train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
+from .training_loop import TrainLoop
+from ..data_loaders.get_data import get_dataset_loader
+from ..utils.model_util import create_model_and_diffusion, load_model_wo_clip, load_pretrained_mdm, load_split_mdm
+from .train_platforms import ClearmlPlatform, TensorboardPlatform, NoPlatform  # required for the eval operation
 import torch
 
 def main():
diff --git a/train/training_loop.py b/train/training_loop.py
index 19752a3..990597e 100644
--- a/train/training_loop.py
+++ b/train/training_loop.py
@@ -11,14 +11,14 @@ from torch.optim import AdamW
 
 from diffusion import logger
 from utils import dist_util
-from diffusion.fp16_util import MixedPrecisionTrainer
-from diffusion.resample import LossAwareSampler, UniformSampler
+from ..diffusion.fp16_util import MixedPrecisionTrainer
+from ..diffusion.resample import LossAwareSampler, UniformSampler
 from tqdm import tqdm
-from diffusion.resample import create_named_schedule_sampler
-from data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
+from ..diffusion.resample import create_named_schedule_sampler
+from ..data_loaders.humanml.networks.evaluator_wrapper import EvaluatorMDMWrapper
 from eval import eval_humanml_double_take, eval_multi
-from data_loaders.get_data import get_dataset_loader
-from utils.misc import load_model_wo_clip
+from ..data_loaders.get_data import get_dataset_loader
+from ..utils.misc import load_model_wo_clip
 
 
 # For ImageNet experiments, this was a good default value.
diff --git a/utils/config.py b/utils/config.py
index 909a2e1..3f45a11 100644
--- a/utils/config.py
+++ b/utils/config.py
@@ -1,24 +1,24 @@
 import os
+from os.path import dirname, join
 
-SMPL_DATA_PATH = "./body_models/smpl"
+SMPL_DATA_PATH = join(dirname(__file__), "..", "./body_models/smpl")
 
 SMPL_KINTREE_PATH = os.path.join(SMPL_DATA_PATH, "kintree_table.pkl")
 SMPL_MODEL_PATH = os.path.join(SMPL_DATA_PATH, "SMPL_NEUTRAL.pkl")
-JOINT_REGRESSOR_TRAIN_EXTRA = os.path.join(SMPL_DATA_PATH, 'J_regressor_extra.npy')
+JOINT_REGRESSOR_TRAIN_EXTRA = os.path.join(SMPL_DATA_PATH, "J_regressor_extra.npy")
 
 ROT_CONVENTION_TO_ROT_NUMBER = {
-    'legacy': 23,
-    'no_hands': 21,
-    'full_hands': 51,
-    'mitten_hands': 33,
+    "legacy": 23,
+    "no_hands": 21,
+    "full_hands": 51,
+    "mitten_hands": 33,
 }
 
-GENDERS = ['neutral', 'male', 'female']
+GENDERS = ["neutral", "male", "female"]
 NUM_BETAS = 10
 
 
-SMPLH_AMASS_PATH = './dataset/AMASS/models/smplh'
-SMPLH_AMASS_PATH = './dataset/AMASS/models/smplh'
+SMPLH_AMASS_PATH = join(dirname(__file__), "..", "./dataset/AMASS/models/smplh")
 SMPLH_AMASS_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, "neutral/model.npz")
 SMPLH_AMASS_MALE_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, "male/model.npz")
 SMPLH_AMASS_FEMALE_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, "female/model.npz")
@@ -27,4 +27,3 @@ SMPLX_DATA_PATH = "models/smplx/"
 SMPLX_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, "SMPLX_NEUTRAL.pkl")
 SMPLX_MALE_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, "SMPLX_MALE.pkl")
 SMPLX_FEMALE_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, "SMPLX_FEMALE.pkl")
-
diff --git a/utils/model_util.py b/utils/model_util.py
index 74b32bd..b30e421 100644
--- a/utils/model_util.py
+++ b/utils/model_util.py
@@ -1,116 +1,165 @@
 import torch
-from diffusion.inpainting_gaussian_diffusion import InpaintingGaussianDiffusion
-from model.cfg_sampler import wrap_model
-from model.comMDM import ComMDM
-from model.mdm import MDM
-from model.DoubleTake_MDM import doubleTake_MDM
-from diffusion import gaussian_diffusion as gd
-from diffusion.respace import SpacedDiffusion, space_timesteps
-from model.model_blending import ModelBlender
-
-def load_model_blending_and_diffusion(args_list, data, device, ModelClass=MDM, DiffusionClass=gd.GaussianDiffusion):
+from priorMDM.model.cfg_sampler import wrap_model
+from priorMDM.model.mdm import MDM
+from priorMDM.diffusion import gaussian_diffusion as gd
+from priorMDM.diffusion.respace import SpacedDiffusion, space_timesteps
+from priorMDM.model.model_blending import ModelBlender
+from os.path import join, dirname
+import logging
+
+logger = logging.getLogger(__name__)
+
+
+def load_model_blending_and_diffusion(
+    args_list, data, device, ModelClass=MDM, DiffusionClass=gd.GaussianDiffusion
+):
     models = [load_model(args, data, device, ModelClass)[0] for args in args_list]
-    model = ModelBlender(models, [1/len(models)]*len(models)) if len(models) > 1 else models[0]
-    
-    _, diffusion = create_model_and_diffusion(args_list[0], data, DiffusionClass=DiffusionClass)
+    model = (
+        ModelBlender(models, [1 / len(models)] * len(models))
+        if len(models) > 1
+        else models[0]
+    )
+
+    _, diffusion = create_model_and_diffusion(
+        args_list[0], data, DiffusionClass=DiffusionClass
+    )
     return model, diffusion
 
-def load_model(args, data, device, ModelClass=MDM):
-    model, diffusion = create_model_and_diffusion(args, data, ModelClass=ModelClass)
-    model_path = args.model_path
-    print(f"Loading checkpoints from [{model_path}]...")
-    state_dict = torch.load(model_path, map_location='cpu')
+
+def load_model(args, device, ModelClass=MDM):
+    model, diffusion = create_model_and_diffusion(args, ModelClass=ModelClass)
+    model_path = join(dirname(__file__), "..", args.model_path)
+    logger.info(f"Loading checkpoints from [{model_path}]...")
+    state_dict = torch.load(model_path, map_location="cpu")
     load_model_wo_clip(model, state_dict)
     model.to(device)
     model.eval()  # disable random masking
     model = wrap_model(model, args)
     return model, diffusion
 
+
 def load_model_wo_clip(model, state_dict):
     missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
-    if 't_pos_encoder.pe' in missing_keys:
-        missing_keys.remove('t_pos_encoder.pe')
-    if 't_pos_encoder.pe' in unexpected_keys:
-        unexpected_keys.remove('t_pos_encoder.pe')
+    if "t_pos_encoder.pe" in missing_keys:
+        missing_keys.remove("t_pos_encoder.pe")
+    if "t_pos_encoder.pe" in unexpected_keys:
+        unexpected_keys.remove("t_pos_encoder.pe")
     assert len(unexpected_keys) == 0
-    assert all([k.startswith('clip_model.') for k in missing_keys])
+    assert all([k.startswith("clip_model.") for k in missing_keys])
+
 
 def load_pretrained_mdm(model, state_dict):
     missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
     assert len(unexpected_keys) == 0
-    assert all([k.startswith('clip_model.') or k.startswith('multi_person.') for k in missing_keys])
+    assert all(
+        [
+            k.startswith("clip_model.") or k.startswith("multi_person.")
+            for k in missing_keys
+        ]
+    )
 
 
 def load_split_mdm(model, state_dict, cutting_point):
     new_state_dict = {}
-    orig_trans_prefix = 'seqTransEncoder.'
+    orig_trans_prefix = "seqTransEncoder."
     for k, v in state_dict.items():
         if k.startswith(orig_trans_prefix):
-            orig_layer = int(k.split('.')[2])
-            orig_suffix = '.'.join(k.split('.')[3:])
-            target_split = 'seqTransEncoder_start.' if orig_layer < cutting_point else 'seqTransEncoder_end.'
-            target_layer = orig_layer if orig_layer < cutting_point else orig_layer - cutting_point
-            new_k = target_split + 'layers.' + str(target_layer) + '.' + orig_suffix
+            orig_layer = int(k.split(".")[2])
+            orig_suffix = ".".join(k.split(".")[3:])
+            target_split = (
+                "seqTransEncoder_start."
+                if orig_layer < cutting_point
+                else "seqTransEncoder_end."
+            )
+            target_layer = (
+                orig_layer if orig_layer < cutting_point else orig_layer - cutting_point
+            )
+            new_k = target_split + "layers." + str(target_layer) + "." + orig_suffix
             new_state_dict[new_k] = v
         else:
             new_state_dict[k] = v
     missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
     assert len(unexpected_keys) == 0
-    assert all([k.startswith('clip_model.') or k.startswith('multi_person.') for k in missing_keys])
+    assert all(
+        [
+            k.startswith("clip_model.") or k.startswith("multi_person.")
+            for k in missing_keys
+        ]
+    )
 
 
-def create_model_and_diffusion(args, data, ModelClass=MDM, DiffusionClass=SpacedDiffusion):
-    model = ModelClass(**get_model_args(args, data))
+def create_model_and_diffusion(args, ModelClass=MDM, DiffusionClass=SpacedDiffusion):
+    model = ModelClass(**get_model_args(args))
     diffusion = create_gaussian_diffusion(args, DiffusionClass)
     return model, diffusion
 
-def get_model_args(args, data):
+
+def get_model_args(args):
 
     # default args
-    clip_version = 'ViT-B/32'
-    action_emb = 'tensor'
-    cond_mode = 'text' if args.dataset in ['humanml', 'babel', 'pw3d'] else 'action'
-    if hasattr(data.dataset, 'num_actions'):
-        num_actions = data.dataset.num_actions
-    else:
-        num_actions = 1
+    clip_version = "ViT-B/32"
+    action_emb = "tensor"
+    cond_mode = "text"
+    num_actions = 1
 
     # SMPL defaults
-    data_rep = 'rot6d'
+    data_rep = "rot6d"
     njoints = 25
     nfeats = 6
 
-    if args.dataset in ['humanml', 'pw3d']:
-        data_rep = 'hml_vec'
+    if args.dataset in ["humanml", "pw3d"]:
+        data_rep = "hml_vec"
         njoints = 263
         nfeats = 1
-    elif args.dataset == 'babel':
-        data_rep = 'rot6d'
+    elif args.dataset == "babel":
+        data_rep = "rot6d"
         njoints = 135
         nfeats = 1
     else:
-        raise TypeError(f'dataset {args.dataset} is not currently supported')
-
-    return {'modeltype': '', 'njoints': njoints, 'nfeats': nfeats, 'num_actions': num_actions,
-            'translation': True, 'pose_rep': 'rot6d', 'glob': True, 'glob_rot': True,
-            'latent_dim': args.latent_dim, 'ff_size': 1024, 'num_layers': args.layers, 'num_heads': 4,
-            'dropout': 0.1, 'activation': "gelu", 'data_rep': data_rep, 'cond_mode': cond_mode,
-            'cond_mask_prob': args.cond_mask_prob, 'action_emb': action_emb, 'arch': args.arch,
-            'emb_trans_dec': args.emb_trans_dec, 'clip_version': clip_version, 'dataset': args.dataset,
-            'diffusion-steps': args.diffusion_steps, 'batch_size': args.batch_size, 'use_tta': args.use_tta,
-            'trans_emb': args.trans_emb, 'concat_trans_emb': args.concat_trans_emb, 'args': args}
+        raise TypeError(f"dataset {args.dataset} is not currently supported")
+
+    return {
+        "modeltype": "",
+        "njoints": njoints,
+        "nfeats": nfeats,
+        "num_actions": num_actions,
+        "translation": True,
+        "pose_rep": "rot6d",
+        "glob": True,
+        "glob_rot": True,
+        "latent_dim": args.latent_dim,
+        "ff_size": 1024,
+        "num_layers": args.layers,
+        "num_heads": 4,
+        "dropout": 0.1,
+        "activation": "gelu",
+        "data_rep": data_rep,
+        "cond_mode": cond_mode,
+        "cond_mask_prob": args.cond_mask_prob,
+        "action_emb": action_emb,
+        "arch": args.arch,
+        "emb_trans_dec": args.emb_trans_dec,
+        "clip_version": clip_version,
+        "dataset": args.dataset,
+        "diffusion-steps": args.diffusion_steps,
+        "batch_size": args.batch_size,
+        "use_tta": args.use_tta,
+        "trans_emb": args.trans_emb,
+        "concat_trans_emb": args.concat_trans_emb,
+        "args": args,
+    }
 
 
 def create_gaussian_diffusion(args, DiffusionClass=SpacedDiffusion):
     # default params
     predict_xstart = True  # we always predict x_start (a.k.a. x0), that's our deal!
     steps = args.diffusion_steps
-    scale_beta = 1.  # no scaling
-    timestep_respacing = ''  # can be used for ddim sampling, we don't use it.
+    scale_beta = 1.0  # no scaling
+    timestep_respacing = ""  # can be used for ddim sampling, we don't use it.
     learn_sigma = False
     rescale_timesteps = False
-    
-    print(f"number of diffusion-steps: {steps}")
+
+    logger.info(f"Number of diffusion-steps: {steps}")
 
     betas = gd.get_named_beta_schedule(args.noise_schedule, steps, scale_beta)
     loss_type = gd.LossType.MSE
@@ -118,7 +167,7 @@ def create_gaussian_diffusion(args, DiffusionClass=SpacedDiffusion):
     if not timestep_respacing:
         timestep_respacing = [steps]
 
-    if hasattr(args, 'multi_train_mode'):
+    if hasattr(args, "multi_train_mode"):
         multi_train_mode = args.multi_train_mode
     else:
         multi_train_mode = None
@@ -145,4 +194,4 @@ def create_gaussian_diffusion(args, DiffusionClass=SpacedDiffusion):
         lambda_fc=args.lambda_fc,
         batch_size=args.batch_size,
         multi_train_mode=multi_train_mode,
-    )
\ No newline at end of file
+    )
diff --git a/utils/sampling_utils.py b/utils/sampling_utils.py
index 10c2d51..51179b7 100644
--- a/utils/sampling_utils.py
+++ b/utils/sampling_utils.py
@@ -1,6 +1,6 @@
 from copy import deepcopy
 import torch
-from utils import dist_util
+from priorMDM.utils import dist_util
 
 
 def unfold_sample_arb_len(sample, handshake_size, step_sizes, final_n_frames, model_kwargs):
@@ -166,4 +166,3 @@ def double_take_arb_len(args, diffusion, model, model_kwargs, n_frames, eval_mod
         model_kwargs['y']['text'].append(last_text)
 
     return samples_per_rep_list, samples_type
-
diff --git a/visualize/simplify_loc2rot.py b/visualize/simplify_loc2rot.py
index 5d3d441..ff6ca31 100644
--- a/visualize/simplify_loc2rot.py
+++ b/visualize/simplify_loc2rot.py
@@ -4,7 +4,7 @@ import torch
 from visualize.joints2smpl.src import config
 import smplx
 import h5py
-from visualize.joints2smpl.src.smplify import SMPLify3D
+from .joints2smpl.src.smplify import SMPLify3D
 from tqdm import tqdm
 import utils.rotation_conversions as geometry
 import argparse
diff --git a/visualize/vis_utils.py b/visualize/vis_utils.py
index 05728b3..98870ce 100644
--- a/visualize/vis_utils.py
+++ b/visualize/vis_utils.py
@@ -1,9 +1,9 @@
-from model.rotation2xyz import Rotation2xyz
+from ..model.rotation2xyz import Rotation2xyz
 import numpy as np
 from trimesh import Trimesh
 import os
 import torch
-from visualize.simplify_loc2rot import joints2smpl
+from .simplify_loc2rot import joints2smpl
 
 class npy2obj:
     def __init__(self, npy_path, sample_idx, rep_idx, device=0, cuda=True):
